<!doctype html><html lang=en-us><head><meta charset=utf-8><title>Running ownCloud in Kubernetes with Rook Ceph Storage - Part 2 | Edenmal - Sysadmin Garden of Eden</title>
<meta name=author content="Alexander Trost"><meta name=description content="How to run ownCloud in Kubernetes with using Rook for a Ceph Cluster."><meta name=twitter:site content="@galexrt"><meta name=twitter:title content="Running ownCloud in Kubernetes with Rook Ceph Storage - Part 2 | Edenmal - Sysadmin Garden of Eden"><meta name=twitter:card content="summary"><meta name=twitter:image content="/images/avatar.png"><meta name=twitter:description content="How to run ownCloud in Kubernetes with using Rook for a Ceph Cluster."><meta property="og:type" content="article"><meta property="og:title" content="Running ownCloud in Kubernetes with Rook Ceph Storage - Part 2"><meta property="og:description" content="How to run ownCloud in Kubernetes with using Rook for a Ceph Cluster."><meta property="og:url" content="https://edenmal.moe/post/2020/Running-ownCloud-in-Kubernetes-with-Rook-Ceph-Storage-Part-2/"><meta property="og:image" content="/images/avatar.png"><meta name=generator content="Hugo 0.120.4"><link rel=canonical href=https://edenmal.moe/post/2020/Running-ownCloud-in-Kubernetes-with-Rook-Ceph-Storage-Part-2/><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=format-detection content="telephone=no,email=no,adress=no"><meta http-equiv=Cache-Control content="no-transform"><meta name=robots content="index,follow"><meta name=referrer content="origin-when-cross-origin"><meta name=theme-color content="#4d4ac7"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="black"><meta name=apple-mobile-web-app-title content="Edenmal"><meta name=msapplication-tooltip content="Edenmal"><meta name=msapplication-navbutton-color content="#4d4ac7"><meta name=msapplication-TileColor content="#4d4ac7"><meta name=msapplication-TileImage content="/icons/icon-144x144.png"><link rel=icon href=https://edenmal.moe/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://edenmal.moe/icons/icon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://edenmal.moe/icons/icon-32x32.png><link rel=icon sizes=192x192 href=https://edenmal.moe/icons/icon-192x192.png><link rel=apple-touch-icon href=https://edenmal.moe/icons/icon-152x152.png><link rel=manifest href=https://edenmal.moe/manifest.json><link rel=preload href=https://edenmal.moe/styles/main-rendered.min.css as=style><link rel=preload href=https://edenmal.moe/images/avatar.png as=image><link rel=preload href=https://edenmal.moe/images/grey-prism.svg as=image><style>body{background-color:#f4f3f1;background-image:url(/images/grey-prism.svg);background-repeat:repeat;background-attachment:fixed}</style><link rel=stylesheet href=https://edenmal.moe/styles/main-rendered.min.css><link rel=stylesheet href=https://edenmal.moe/cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe.min.min.css><link rel=stylesheet href=https://edenmal.moe/cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/default-skin/default-skin.min.min.css><script src=https://edenmal.moe/cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe.min.min.819c76b4bba0f8b8c568f6e4860173f1d16e5046c01b008f4705ae823b5766e1.js integrity="sha256-gZx2tLug+LjFaPbkhgFz8dFuUEbAGwCPRwWugjtXZuE="></script><script src=https://edenmal.moe/cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe-ui-default.min.min.2fcfc43be15566e9378af0e8247ba57fec8d4ee95c5dba15cf97e84d6c2115bc.js integrity="sha256-L8/EO+FVZuk3ivDoJHulf+yNTulcXboVz5foTWwhFbw="></script><script src=https://edenmal.moe/cdnjs.cloudflare.com/ajax/libs/jquery/1.12.4/jquery.min.min.77ec5e7d7f844e9b6c2413a352b48194493a65c75d94d5cd9679a225c2da773c.js integrity="sha256-d+xefX+ETptsJBOjUrSBlEk6ZcddlNXNlnmiJcLadzw="></script><script src=https://edenmal.moe/scripts/pswp-init.min.299a530b67d8be41667aa9ed78920e272e970b0ac8ab7540ba3a7c1382b52de2.js integrity="sha256-KZpTC2fYvkFmeqnteJIOJy6XCwrIq3VAujp8E4K1LeI="></script></head><body><div class=suspension><a role=button aria-label="Go to top" title="Go to top" class="to-top is-hide"><span class=icofont-caret-up aria-hidden=true></span></a>
<a role=button aria-label="Print page" title="Print page" class=print-page onclick=window.print()><span class=icofont-print aria-hidden=true></span></a></div><header class=site-header><a class=avatara href=https://edenmal.moe/><img class=avatar src=https://edenmal.moe/images/avatar.png alt=Avatar></a><h2 class=title><a href=https://edenmal.moe/>Edenmal</a></h2><p class=subtitle>Sysadmin Garden of Eden</p><button class=menu-toggle type=button aria-label="Main Menu" aria-expanded=false tab-index=0>
<span class=icofont-navigation-menu aria-hidden=true></span></button><nav class="site-menu collapsed"><h2 class=offscreen>Main Menu</h2><ul class=menu-list><li class="menu-item
is-active"><a href=https://edenmal.moe/>Home</a></li><li class=menu-item><a href=https://edenmal.moe/about/>About</a></li><li class=menu-item><a href=https://edenmal.moe/tags/>Tags</a></li><li class=menu-item><a href=https://edenmal.moe/events/>Events</a></li><li class=menu-item><a href=https://edenmal.moe/site-notice/>Impressum</a></li></ul></nav><nav class="social-menu collapsed"><h2 class=offscreen>Social Networks</h2><ul class=social-list><li class=social-item><a href=mailto:me@galexrt.moe title=Email aria-label=Email><span class=icofont-envelope aria-hidden=true></span></a></li><li class=social-item><a href=https://galexrt.moe/ rel=me title=Homepage aria-label=Homepage><span class=icofont-page aria-hidden=true></span></a></li><li class=social-item><a href=//github.com/galexrt rel=me title=GitHub aria-label=GitHub><span class=icofont-github aria-hidden=true></span></a></li><li class=social-item><a href=//twitter.com/galexrt rel=me title=Twitter aria-label=Twitter><span class=icofont-twitter aria-hidden=true></span></a></li><li class=social-item><a href=//www.linkedin.com/in/alexander-trost rel=me title=LinkedIn aria-label=LinkedIn><span class=icofont-linkedin aria-hidden=true></span></a></li><li class=social-item><a href=//www.xing.com/profile/Alexander_Trost18 rel=me title=XING aria-label=XING><span class=icofont-xing aria-hidden=true></span></a></li><li class=social-item><a rel=alternate type=application/rss+xml href=https://edenmal.moe/index.xml title=RSS aria-label=RSS><span class=icofont-rss aria-hidden=true></span></a></li></ul></nav></header><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><section class="main post-detail"><header class=post-header><h1 class=post-title>Running ownCloud in Kubernetes with Rook Ceph Storage - Part 2</h1><p class=post-meta>Author Alexander Trost · Read Time 13 minutes · Created Mon Jan 6 14:56:41 2020 · Updated Fri Mar 25 21:33:36 2022</p></header><article class=post-content><h2>Table of Contents</h2><nav id=TableOfContents><ul><li><a href=#preparations>Preparations</a><ul><li><a href=#kubernetes-cluster-access>Kubernetes Cluster Access</a></li><li><a href=#kubernetes-cluster>Kubernetes Cluster</a></li></ul></li><li><a href=#rook-ceph-storage>Rook Ceph Storage</a><ul><li><a href=#operator>Operator</a></li><li><a href=#ceph-cluster>Ceph Cluster</a></li><li><a href=#block-storage-rbd>Block storage (RBD)</a></li><li><a href=#cephfs>CephFS</a></li><li><a href=#toolbox>Toolbox</a></li><li><a href=#summary>Summary</a></li></ul></li><li><a href=#postgresql>PostgreSQL</a></li><li><a href=#redis>Redis</a></li><li><a href=#owncloud>ownCloud</a><ul><li><a href=#further-points>Further points</a></li><li><a href=#updating-owncloud-in-kubernetes>Updating ownCloud in Kubernetes</a></li></ul></li><li><a href=#summary-1>Summary</a></li></ul></nav><hr><p>This a cross post of a post I wrote for the <a href=https://owncloud.org/news/>ownCloud Blog</a>, the original post can be found here: <a href=https://owncloud.org/news/running-owncloud-in-kubernetes-with-rook-ceph-storage-step-by-step/>Running ownCloud in Kubernetes With Rook Ceph Storage – Step by Step</a>.</p><p>Thanks to them for allowing me to write and publish the post on their blog!</p><hr><h2 id=preparations>Preparations</h2><p>Let&rsquo;s prepare for the Kubernetes madness!</p><h3 id=kubernetes-cluster-access>Kubernetes Cluster Access</h3><p>As written in the first part, it is expected that you have (admin) access to a Kubernetes cluster already.</p><p>If you don&rsquo;t have a Kubernetes cluster, you can try using the following projects <a href=https://github.com/xetys/hetzner-kube>xetys/hetzner-kube on GitHub</a>, <a href=https://kubespray.io/>Kubespray</a> and <a href=https://kubernetes.io/docs/setup/>others (Kubernetes documentation)</a>.</p><p>minikube is not enough when started with the default resources, be sure to give minikube extra resources otherwise you will run into problems! Be sure to add the following flags to the <code>minikube start</code> command: <code>--memory=4096 --cpus=3 --disk-size=40g</code>.</p><p>You should have <code>cluster-admin</code> access to the Kubernetes cluster! Other access can also work, but due to the nature of objects that are created along the way it is easier to have the <code>cluster-admin</code> access.</p><h3 id=kubernetes-cluster>Kubernetes Cluster</h3><h4 id=ingress-controller>Ingress Controller</h4><p><strong>WARNING</strong> Only follow this section, if your Kubernetes cluster does not have an Ingress controller yet.</p><p>We are going to install the Kubernetes NGINX Ingress Controller.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span># Taken from https://github.com/kubernetes/ingress-nginx/blob/master/deploy/static/mandatory.yaml
</span></span><span style=display:flex><span><span style=color:#44475a>kubectl apply -f ingress-nginx/
</span></span></span></code></pre></div><p>The instructions shown here are for an environment without <code>LoadBalancer</code> Service type support (e.g., bare metal, &ldquo;normal&rdquo; VM provider, not cloud), for installation instructions for other environments checkout <a href=https://kubernetes.github.io/ingress-nginx/deploy/>Installation Guide - NGINX Ingress Controller</a>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span># Taken from <span style=color:#6272a4># Taken from https://github.com/kubernetes/ingress-nginx/blob/master/deploy/static/provider/baremetal/service-nodeport.yaml</span>
</span></span><span style=display:flex><span><span style=color:#44475a>kubectl apply -f ingress-nginx/service-nodeport.yaml
</span></span></span></code></pre></div><p>As these are bare metal installation instructions, the NGINX Ingress controller will be available through a Service of type <code>NodePort</code>. This Service type exposes one or more ports on all Nodes in the Kubernetes cluster.</p><p>To get that port run:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>$ kubectl get -n ingress-nginx service ingress-nginx
</span></span><span style=display:flex><span><span style=color:#44475a>NAME            TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
</span></span></span><span style=display:flex><span><span style=color:#44475a>ingress-nginx   NodePort   10.108.254.160   &lt;none&gt;        80:30512/TCP,443:30243/TCP   23m
</span></span></span></code></pre></div><p>In that output you can see the NodePorts for HTTP and HTTPS on which you can connect to the NGINX Ingress controller and ownCloud later.</p><p>Though as written you probably want to look into a more &ldquo;solid&rdquo; way to expose the NGINX Ingress controller(s), for bare metal where there is no Kubernetes LoadBalancer integration one can consider using <code>hostNetwork</code> option for that: <a href=https://kubernetes.github.io/ingress-nginx/deploy/baremetal/#via-the-host-network>Bare-metal considerations - NGINX Ingress Controller</a>.</p><h4 id=namespaces>Namespaces</h4><p>Through the whole installation we will create 4 Namespaces:</p><ul><li><code>rook-ceph</code> - For the Rook run Ceph cluster + the Rook Ceph operator (will be created in <a href=#rook-ceph-storage>Rook Ceph storage</a>).</li><li><code>owncloud</code> - For ownCloud and the other operators, such as Zalando&rsquo;s Postgres Operator and KubeDB for Redis.</li><li><code>ingress-nginx</code> - If you don&rsquo;t have an Ingress controller running yet, the namespace is used for the Ingress NGINX controller (it is already created in the <a href=#ingress-controller>Ingress Controller steps</a>).</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#44475a>kubectl create -f namespaces.yaml
</span></span></span></code></pre></div><h2 id=rook-ceph-storage>Rook Ceph Storage</h2><p>Now on to running Ceph in Kubernetes, using the <a href=https://rook.io/>Rook.io project</a>.</p><p>In the following sections make sure to use the available <code>-test</code> suffixed files if you have less than 3 Nodes which are available to any application / Pod (e.g., depending on your cluster the masters are not available for Pods).
(You can change that, for that be sure to dig into the <code>CephCluster</code> object&rsquo;s <code>spec.placement.tolerations</code> and the Operator environment variables for the discover and agent daemons. Running application Pods on the masters is not recommended though)</p><h3 id=operator>Operator</h3><p>The operator will take care of starting up the Ceph components one by one and also preparing of disks and health checking.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#44475a>kubectl create -f rook-ceph/common.yaml
</span></span></span><span style=display:flex><span><span style=color:#44475a>kubectl create -f rook-ceph/operator.yaml
</span></span></span></code></pre></div><p>You can check on the Pods to see how it looks:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>$ kubectl get -n rook-ceph pod
</span></span><span style=display:flex><span><span style=color:#44475a>NAME                                  READY   STATUS    RESTARTS   AGE
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-agent-cbrgv                 1/1     Running   0          90s
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-agent-wfznr                 1/1     Running   0          90s
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-agent-zhgg7                 1/1     Running   0          90s
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-operator-6897f5c696-j724m   1/1     Running   0          2m18s
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-discover-jg798                   1/1     Running   0          90s
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-discover-kfxc8                   1/1     Running   0          90s
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-discover-qbhfs                   1/1     Running   0          90s
</span></span></span></code></pre></div><p>The <code>rook-discover-*</code> Pods are each one on each Node of your Kubernetes cluster, as they are discovering the disks of the Nodes so the operator can plan the actions for a given <code>CephCluster</code> object which comes up next.</p><h3 id=ceph-cluster>Ceph Cluster</h3><p>This is the definition of Ceph cluster that will be created in Kubernetes. It contains the lists and options on which disks to use and on which Nodes.</p><p>If you wanna see some example CephCluster objects to see what is possible, be sure to checkout <a href=https://rook.io/docs/rook/v1.0/ceph-cluster-crd.html>Rook v1.0 Documentation - CephCluster CRD</a>.</p><p><strong>INFO</strong> Use the <code>cluster-test.yaml</code> when your Kubernetes cluster has less than 3 schedulable Nodes (e.g., minikube)!
When using the <code>cluster-test.yaml</code> only one <code>mon</code> is started. If that mon is down for whatever reason, the Ceph Cluster will come to a halt to prevent any data &ldquo;corruption&rdquo;.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>$ kubectl create -f rook-ceph/cluster.yaml
</span></span></code></pre></div><p>This will now cause the operator to start the Ceph cluster after the specifications in the CephCluster object.</p><p>To see which Pods have already been created by the operator, you can run (output example from a three node cluster):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>$ kubectl get -n rook-ceph pod
</span></span><span style=display:flex><span><span style=color:#44475a>NAME                                                     READY   STATUS      RESTARTS   AGE
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-agent-cbrgv                                    1/1     Running     0          11m
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-agent-wfznr                                    1/1     Running     0          11m
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-agent-zhgg7                                    1/1     Running     0          11m
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-mgr-a-77fc54c489-66mpd                         1/1     Running     0          6m45s
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-mon-a-68b94cd66-m48lm                          1/1     Running     0          8m6s
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-mon-b-7b679476f-mc7wj                          1/1     Running     0          8m
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-mon-c-b5c468c94-f8knt                          1/1     Running     0          7m54s
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-operator-6897f5c696-j724m                      1/1     Running     0          11m
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-osd-0-5c8d8fcdd-m4gl7                          1/1     Running     0          5m55s
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-osd-1-67bfb7d647-vzmpv                         1/1     Running     0          5m56s
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-osd-2-c8c55548f-ws8sl                          1/1     Running     0          5m11s
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-osd-prepare-owncloudrookceph-worker-01-svvz9   0/2     Completed   0          6m7s
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-osd-prepare-owncloudrookceph-worker-02-mhvf2   0/2     Completed   0          6m7s
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-osd-prepare-owncloudrookceph-worker-03-nt2gs   0/2     Completed   0          6m7s
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-discover-jg798                                      1/1     Running     0          11m
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-discover-kfxc8                                      1/1     Running     0          11m
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-discover-qbhfs                                      1/1     Running     0          11m
</span></span></span></code></pre></div><h3 id=block-storage-rbd>Block storage (RBD)</h3><p>Before creating the CephFS filesystem, let&rsquo;s create a block storage pool with a StorageClass.
The StorageClass is for the PostgreSQL and if you want even the Redis cluster.</p><p><strong>INFO</strong> Use the <code>storageclass-test.yaml</code> when your Kubernetes cluster has less than 3 schedulable Nodes!</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#44475a>kubectl create -f rook-ceph/storageclass.yaml
</span></span></span></code></pre></div><p>In case of a block storage Pool there are no additional Pods that will be started, we&rsquo;ll verify that the block storage Pool has been created in the <a href=#toolbox>Toolbox section</a>.</p><p>One more thing to do is, to set the created StorageClass as default in the Kubernetes cluster by running the following command:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#44475a>kubectl patch storageclass rook-ceph-block -p &#39;{&#34;metadata&#34;: {&#34;annotations&#34;:{&#34;storageclass.kubernetes.io/is-default-class&#34;:&#34;true&#34;}}}&#39;
</span></span></span></code></pre></div><p>Now you are ready to move onto the storage for the actual data to be stored in ownCloud!</p><h3 id=cephfs>CephFS</h3><p>CephFS is the filesystem that Ceph offers, with its POSIX compliance it is a perfect fit to be used with ownCloud.</p><p><strong>INFO</strong> Use the <code>filesystem-test.yaml</code> when your Kubernetes cluster has less than 3 schedulable Nodes!</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#44475a>kubectl create -f rook-ceph/filesystem.yaml
</span></span></span></code></pre></div><p>Creation of the CephFS will cause, so called MDS daemons, MDS Pods to be started.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#44475a>kubectl get -n rook-ceph pod
</span></span></span><span style=display:flex><span><span style=color:#44475a>NAME                                    READY   STATUS      RESTARTS   AGE
</span></span></span><span style=display:flex><span><span style=color:#44475a>[...]
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-mds-myfs-a-747b75bdc7-9nzwx                    1/1     Running     0          11s
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-mds-myfs-b-76b9fcc8cc-md8bz                    1/1     Running     0          10s
</span></span></span><span style=display:flex><span><span style=color:#44475a>[...]
</span></span></span></code></pre></div><h3 id=toolbox>Toolbox</h3><p>This will create a Pod which will allow us to run Ceph commands. It will be use to quickly check the Ceph clusters status.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#44475a>kubectl create -f rook-ceph/toolbox.yaml
</span></span></span><span style=display:flex><span><span style=color:#44475a></span># Wait <span style=color:#ff79c6>for</span> the Pod to be <span style=color:#f1fa8c>`</span>Running<span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span><span style=color:#44475a>kubectl get -n rook-ceph pod -l &#34;app=rook-ceph-tools&#34;
</span></span></span><span style=display:flex><span><span style=color:#44475a>NAME                                    READY   STATUS      RESTARTS   AGE
</span></span></span><span style=display:flex><span><span style=color:#44475a>[...]
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-tools-5966446d7b-nrw5n                         1/1     Running     0          10s
</span></span></span><span style=display:flex><span><span style=color:#44475a>[...]
</span></span></span></code></pre></div><p>Now use <code>kubectl exec</code> to enter the Rook Ceph Toolbox Pod:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#44475a>kubectl exec -n rook-ceph -it $(kubectl get -n rook-ceph pod -l &#34;app=rook-ceph-tools&#34; -o jsonpath=&#39;{.items[0].metadata.name}&#39;) bash
</span></span></span></code></pre></div><p>In the Rook Ceph Toolbox Pod, run the following command to get the Ceph cluster health status (example output from a 7 Node Kubernetes Rook Ceph cluster):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>$ ceph -s
</span></span><span style=display:flex><span><span style=color:#44475a> cluster:
</span></span></span><span style=display:flex><span><span style=color:#44475a>   id:     f8492cd9-3d14-432c-b681-6f73425d6851
</span></span></span><span style=display:flex><span><span style=color:#44475a>   health: HEALTH_OK
</span></span></span><span style=display:flex><span><span style=color:#44475a></span>
</span></span><span style=display:flex><span><span style=color:#44475a> services:
</span></span></span><span style=display:flex><span><span style=color:#44475a>   mon: 3 daemons, quorum c,b,a
</span></span></span><span style=display:flex><span><span style=color:#44475a>   mgr: a(active)
</span></span></span><span style=display:flex><span><span style=color:#44475a>   mds: repl-2-1-2/2/2 up  {0=repl-2-1-c=up:active,1=repl-2-1-b=up:active}, 2 up:standby-replay
</span></span></span><span style=display:flex><span><span style=color:#44475a>   osd: 7 osds: 7 up, 7 in
</span></span></span><span style=display:flex><span><span style=color:#44475a></span>
</span></span><span style=display:flex><span><span style=color:#44475a> data:
</span></span></span><span style=display:flex><span><span style=color:#44475a>   pools:   3 pools, 300 pgs
</span></span></span><span style=display:flex><span><span style=color:#44475a>   objects: 1.41 M objects, 4.0 TiB
</span></span></span><span style=display:flex><span><span style=color:#44475a>   usage:   8.2 TiB used, 17 TiB / 25 TiB avail
</span></span></span><span style=display:flex><span><span style=color:#44475a>   pgs:     300 active+clean
</span></span></span><span style=display:flex><span><span style=color:#44475a></span>
</span></span><span style=display:flex><span><span style=color:#44475a> io:
</span></span></span><span style=display:flex><span><span style=color:#44475a>   client:   6.2 KiB/s rd, 1.5 MiB/s wr, 4 op/s rd, 140 op/s wr
</span></span></span></code></pre></div><p>You can also get it by using <code>kubectl</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>$ kubectl get -n rook-ceph cephcluster rook-ceph
</span></span><span style=display:flex><span><span style=color:#44475a>NAME        DATADIRHOSTPATH   MONCOUNT   AGE   STATE     HEALTH
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph   /mnt/sda1/rook    3          14m   Created   HEALTH_OK
</span></span></span></code></pre></div><p>That even shows you some additional information directly through <code>kubectl</code> instead of having to read the <code>ceph -s</code> output.</p><h3 id=summary>Summary</h3><p>This is how it should look Pod wise now in your <code>rook-ceph</code> Namespace (example output from a 3 Node Kubernetes cluster):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>$ kubectl get -n rook-ceph pod
</span></span><span style=display:flex><span><span style=color:#44475a>NAME                                                     READY   STATUS      RESTARTS   AGE
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-agent-cbrgv                                    1/1     Running     0          15m
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-agent-wfznr                                    1/1     Running     0          15m
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-agent-zhgg7                                    1/1     Running     0          15m
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-mds-myfs-a-747b75bdc7-9nzwx                    1/1     Running     0          42s
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-mds-myfs-b-76b9fcc8cc-md8bz                    1/1     Running     0          41s
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-mgr-a-77fc54c489-66mpd                         1/1     Running     0          11m
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-mon-a-68b94cd66-m48lm                          1/1     Running     0          12m
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-mon-b-7b679476f-mc7wj                          1/1     Running     0          2m22s
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-mon-c-b5c468c94-f8knt                          1/1     Running     0          2m6s
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-operator-6897f5c696-j724m                      1/1     Running     0          16m
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-osd-0-5c8d8fcdd-m4gl7                          1/1     Running     0          10m
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-osd-1-67bfb7d647-vzmpv                         1/1     Running     0          10m
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-osd-2-c8c55548f-ws8sl                          1/1     Running     0          9m48s
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-osd-prepare-owncloudrookceph-worker-01-5xpqk   0/2     Completed   0          73s
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-osd-prepare-owncloudrookceph-worker-02-xnl8p   0/2     Completed   0          70s
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-osd-prepare-owncloudrookceph-worker-03-2qggs   0/2     Completed   0          68s
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-ceph-tools-5966446d7b-nrw5n                         1/1     Running     0          8s
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-discover-jg798                                      1/1     Running     0          15m
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-discover-kfxc8                                      1/1     Running     0          15m
</span></span></span><span style=display:flex><span><span style=color:#44475a>rook-discover-qbhfs                                      1/1     Running     0          15m
</span></span></span></code></pre></div><p>The important thing is that the <code>ceph -s</code> output or the <code>kubectl get cephcluster</code> output shows that the <code>health</code> is <code>HEALTH_OK</code> and that you have OSD Pods running (<code>ceph -s</code> output line: <code>osd: 3 osds: 3 up, 3 in</code> (where 3 is basically the amount of OSD Pods).</p><p>Should you not have any OSD Pod, make sure all your Nodes are <code>Ready</code> and schedulable (e.g., no taints preventing &ldquo;normal&rdquo; Pods to run) and make sure to checkout the logs of the <code>rook-ceph-osd-prepare-*</code> and if existing <code>rook-ceph-osd-[0-9]*</code> Pods. If you don&rsquo;t have any Pods related to <code>rook-ceph-osd-*</code> look into the <code>rook-ceph-operator-*</code> logs for error messages, be sure to go over each line to make sure you don&rsquo;t miss an error message.</p><h2 id=postgresql>PostgreSQL</h2><p>Moving on to the PostgreSQL for ownCloud.</p><p>Zalando&rsquo;s PostgreSQL operator does a great job for running PostgreSQL in Kubernetes.</p><p>First thing to create is the PostgreSQL Operator which brings the CustomResourceDefinitions, remember the custom Kubernetes objects, with itself.
Using the Ceph block storage (RBD) we are going to create a redundant PostgreSQL instance for ownCloud to use.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>$ kubectl create -n owncloud -f postgres/postgres-operator.yaml
</span></span><span style=display:flex><span># Check <span style=color:#ff79c6>for</span> the PostgreSQL operator Pod to be created and running
</span></span><span style=display:flex><span>$ kubectl get -n owncloud pod
</span></span><span style=display:flex><span><span style=color:#44475a>NAME                                 READY   STATUS    RESTARTS   AGE
</span></span></span><span style=display:flex><span><span style=color:#44475a>postgres-operator-6464fc9c48-6twrd   1/1     Running   0          5m23s
</span></span></span></code></pre></div><p>That is the operator created, moving on to the PostgreSQL custom resource object that will cause the operator to create a PostgreSQL instance for use in Kubernetes:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span># Make sure the CustomResourceDefinition of the PostgreSQL has been created
</span></span><span style=display:flex><span>$ kubectl get customresourcedefinitions.apiextensions.k8s.io postgresqls.acid.zalan.do
</span></span><span style=display:flex><span><span style=color:#44475a>NAME                        CREATED AT
</span></span></span><span style=display:flex><span><span style=color:#44475a>postgresqls.acid.zalan.do   2019-08-04T10:27:59Z
</span></span></span></code></pre></div><p>The CustomResourceDefinition exists? Perfect, continue with the creation:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#44475a>kubectl create -n owncloud -f postgres/postgres.yaml
</span></span></span></code></pre></div><p>It will take a bit for the two PostgreSQL Pods to appear, but in the end you should have two <code>owncloud-postgres</code> Pods:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>$ kubectl get -n owncloud pod
</span></span><span style=display:flex><span><span style=color:#44475a>NAME                                 READY   STATUS    RESTARTS   AGE
</span></span></span><span style=display:flex><span><span style=color:#44475a>owncloud-postgres-0                  1/1     Running   0          92s
</span></span></span><span style=display:flex><span><span style=color:#44475a>owncloud-postgres-1                  1/1     Running   0          64s
</span></span></span><span style=display:flex><span><span style=color:#44475a>postgres-operator-6464fc9c48-6twrd   1/1     Running   0          7m
</span></span></span></code></pre></div><p><code>owncloud-postgres-0</code> and <code>owncloud-postgres-1</code> in <code>Running</code> status? That looks good.</p><p>Now that the database is running, let&rsquo;s continue to the Redis.</p><h2 id=redis>Redis</h2><p>To run a Redis cluster we need the KubeDB Operator, installing it can done using a bash script or Helm.</p><p>To keep it quick&rsquo;n&rsquo;easy we&rsquo;ll use their bash script for that:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#44475a>curl -fsSL https://raw.githubusercontent.com/kubedb/cli/0.12.0/hack/deploy/kubedb.sh -o kubedb.sh
</span></span></span><span style=display:flex><span><span style=color:#44475a></span># Take a look at the script using, e.g., <span style=color:#f1fa8c>`</span>cat kubedb.sh<span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>#
</span></span><span style=display:flex><span><span style=color:#6272a4># If you are fine with it, run it:</span>
</span></span><span style=display:flex><span><span style=color:#44475a>chmod +x kubedb.sh
</span></span></span><span style=display:flex><span><span style=color:#44475a>./kubedb.sh
</span></span></span><span style=display:flex><span><span style=color:#44475a></span># It will install the KubeDB operator to the cluster in the <span style=color:#f1fa8c>`</span>kube-system<span style=color:#f1fa8c>`</span> Namespace
</span></span></code></pre></div><p>(You can remove the script afterwards: <code>rm kubedb.sh</code>)</p><p>For more information on the bash script and / or the Helm installation, checkout <a href=https://kubedb.com/docs/0.12.0/setup/install/#install-kubedb-operator>KubeDB</a>.</p><p>Moving on to creating the Redis cluster, run:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#44475a>kubectl create -n owncloud -f redis.yaml
</span></span></span></code></pre></div><p>It will take a few seconds for the first Redis Pod(s) to be started, to check that it worked look for Pods with <code>redis-owncloud-</code> in their name:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>$ kubectl get -n owncloud pods
</span></span><span style=display:flex><span><span style=color:#44475a>NAME                                 READY   STATUS    RESTARTS   AGE
</span></span></span><span style=display:flex><span><span style=color:#44475a>owncloud-postgres-0                  1/1     Running   0          6m41s
</span></span></span><span style=display:flex><span><span style=color:#44475a>owncloud-postgres-1                  1/1     Running   0          6m13s
</span></span></span><span style=display:flex><span><span style=color:#44475a>postgres-operator-6464fc9c48-6twrd   1/1     Running   0          12m
</span></span></span><span style=display:flex><span><span style=color:#44475a>redis-owncloud-shard0-0              1/1     Running   0          49s
</span></span></span><span style=display:flex><span><span style=color:#44475a>redis-owncloud-shard0-1              1/1     Running   0          40s
</span></span></span><span style=display:flex><span><span style=color:#44475a>redis-owncloud-shard1-0              1/1     Running   0          29s
</span></span></span><span style=display:flex><span><span style=color:#44475a>redis-owncloud-shard1-1              1/1     Running   0          19s
</span></span></span><span style=display:flex><span><span style=color:#44475a>redis-owncloud-shard2-0              1/1     Running   0          14s
</span></span></span><span style=display:flex><span><span style=color:#44475a>redis-owncloud-shard2-1              1/1     Running   0          10s
</span></span></span></code></pre></div><p>That is how it should like now.</p><h2 id=owncloud>ownCloud</h2><p>Now the final &ldquo;piece&rdquo;, ownCloud.</p><p>The folder <code>owncloud/</code> contains all the manifests we need.</p><ul><li>ConfigMap and Secret for basic configuration of the ownCloud.</li><li>Deployment to get ownCloud Pods running in Kubernetes.</li><li>Service and Ingress to expose ownCloud to the internet.</li><li>CronJob to run the ownCloud cron task execution (e.g., cleanup and others), instead of having the cron run per instance.</li></ul><p>The ownCloud Deployment currently uses a custom built image (<code>galexrt/owncloud-server:latest</code>) which has a fix for a clustered Redis configuration issue (pull request has been opened <a href=https://github.com/owncloud-docker/base/pull/95)>https://github.com/owncloud-docker/base/pull/95)</a>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#44475a>kubectl create -n owncloud -f owncloud/
</span></span></span><span style=display:flex><span><span style=color:#44475a></span># Now we&#39;ll <span style=color:#8be9fd;font-style:italic>wait</span> <span style=color:#ff79c6>for</span> ownCloud to have installed the database to <span style=color:#ff79c6>then</span> scale the ownCloud up to <span style=color:#f1fa8c>`</span>2<span style=color:#f1fa8c>`</span> <span style=color:#ff79c6>(</span>or more <span style=color:#ff79c6>if</span> you want<span style=color:#ff79c6>)</span>
</span></span></code></pre></div><p>The admin username is <code>myowncloudadmin</code> and can be changed in the <code>owncloud/owncloud-configmap.yaml</code> file. Be sure to restart both ownCloud Pods after changing values in the ConfigMaps and Secrets.</p><p>If you want to change the admin password, edit the <code>owncloud/owncloud-secret.yaml</code> file line <code>OWNCLOUD_ADMIN_PASSWORD</code>. The values in a Kubernetes Secret object are base64 encoded (e.g., <code>echo -n YOUR_PASSWORD | base64 -w0</code>)!</p><p>To know when your ownCloud is up&rsquo;n&rsquo;running check the logs, e.g.:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>$ kubectl logs -n owncloud -f owncloud-856fcc4947-crscn
</span></span><span style=display:flex><span><span style=color:#44475a>Creating volume folders...
</span></span></span><span style=display:flex><span><span style=color:#44475a>Creating hook folders...
</span></span></span><span style=display:flex><span><span style=color:#44475a>Waiting for PostgreSQL...
</span></span></span><span style=display:flex><span><span style=color:#44475a>wait-for-it: waiting 180 seconds for owncloud-postgres:5432
</span></span></span><span style=display:flex><span><span style=color:#44475a>wait-for-it: owncloud-postgres:5432 is available after 1 seconds
</span></span></span><span style=display:flex><span><span style=color:#44475a>Removing custom folder...
</span></span></span><span style=display:flex><span><span style=color:#44475a>Linking custom folder...
</span></span></span><span style=display:flex><span><span style=color:#44475a>Removing config folder...
</span></span></span><span style=display:flex><span><span style=color:#44475a>Linking config folder...
</span></span></span><span style=display:flex><span><span style=color:#44475a>Writing config file...
</span></span></span><span style=display:flex><span><span style=color:#44475a>Fixing base perms...
</span></span></span><span style=display:flex><span><span style=color:#44475a>Fixing data perms...
</span></span></span><span style=display:flex><span><span style=color:#44475a>Fixing hook perms...
</span></span></span><span style=display:flex><span><span style=color:#44475a>Installing server database...
</span></span></span><span style=display:flex><span><span style=color:#44475a>ownCloud was successfully installed
</span></span></span><span style=display:flex><span><span style=color:#44475a>ownCloud is already latest version
</span></span></span><span style=display:flex><span><span style=color:#44475a>Writing objectstore config...
</span></span></span><span style=display:flex><span><span style=color:#44475a>Writing php config...
</span></span></span><span style=display:flex><span><span style=color:#44475a>Updating htaccess config...
</span></span></span><span style=display:flex><span><span style=color:#44475a>.htaccess has been updated
</span></span></span><span style=display:flex><span><span style=color:#44475a>Writing apache config...
</span></span></span><span style=display:flex><span><span style=color:#44475a>Enabling webcron background...
</span></span></span><span style=display:flex><span><span style=color:#44475a>Set mode for background jobs to &#39;webcron&#39;
</span></span></span><span style=display:flex><span><span style=color:#44475a>Touching cron configs...
</span></span></span><span style=display:flex><span><span style=color:#44475a>Starting cron daemon...
</span></span></span><span style=display:flex><span><span style=color:#44475a>Starting apache daemon...
</span></span></span><span style=display:flex><span><span style=color:#44475a>[Sun Aug 04 13:26:18.986407 2019] [mpm_prefork:notice] [pid 190] AH00163: Apache/2.4.29 (Ubuntu) configured -- resuming normal operations
</span></span></span><span style=display:flex><span><span style=color:#44475a>[Sun Aug 04 13:26:18.986558 2019] [core:notice] [pid 190] AH00094: Command line: &#39;/usr/sbin/apache2 -f /etc/apache2/apache2.conf -D FOREGROUND&#39;
</span></span></span></code></pre></div><p>The <code>Installing server database...</code> will take some time depending on your network, storage and other factors.</p><p>After the <code>[Sun Aug 04 13:26:18.986558 2019] [core:notice] [pid 190] AH00094: Command line: '/usr/sbin/apache2 -f /etc/apache2/apache2.conf -D FOREGROUND'</code> you should be able to reach your ownCloud instance through the NodePort Service Port (on HTTP) or through the Ingress (default address <code>owncloud.example.com</code>). If you are using the Ingress from the example files, be sure to edit it to use a (sub-) domain pointing to the Ingress controllers in your Kubernetes cluster.</p><p>You now have a ownCloud instance running.</p><h3 id=further-points>Further points</h3><h4 id=https>HTTPS</h4><p>To further improve the experience of running ownCloud in Kubernetes, you will probably want to checkout <a href=https://github.com/jetstack/cert-manager>Jetstack&rsquo;s cert-manager project on GitHub</a> to get yourself Letsencrypt certificates for your Ingress controller.
The <code>cert-manager</code> allows you to request <a href=https://letsencrypt.org/>Let&rsquo;s Encrypt</a> certificates easily through Kubernetes custom objects and keep them uptodate.</p><p>Meaning the ownCloud will then be reachable via HTTPS which combined with the ownCloud encryption makes it pretty secure.</p><p>For more information on using TLS with Kubernetes Ingress, checkout <a href=https://kubernetes.io/docs/concepts/services-networking/ingress/#tls>Ingress - Kubernetes</a>.</p><h4 id=pod-health-checks>Pod Health Checks</h4><p>In the <code>owncloud/owncloud-deployment.yaml</code> there is a <code>readinessProbe</code> and <code>livenessProbe</code> in the Deployment sepc but commented out.
After the ownCloud has been installed and you have verified it is running, you can go ahead and uncomment those lines and use <code>kubectl apply</code> / <code>kubectl replace</code> (don&rsquo;t forget to specify the Namespace <code>-n owncloud</code>).</p><h4 id=upload-filesize>Upload Filesize</h4><p>When changing the upload filesize on the ownCloud instance itself through the environment variables, be sure to also update the Ingress controller with the &ldquo;max upload file size&rdquo;.</p><h4 id=other-configuration-options>Other configuration options</h4><p>When wanting to change config options, you need to provide them through environment variables. The environment variables are given to the ownCloud Deployment in the <code>owncloud/owncloud-configmap.yaml</code>.</p><p>A list of all available environment variables can be found here:</p><ul><li><a href=https://github.com/owncloud-docker/server#available-environment-variables>https://github.com/owncloud-docker/server#available-environment-variables</a></li><li><a href=https://github.com/owncloud-docker/base#available-environment-variables>https://github.com/owncloud-docker/base#available-environment-variables</a></li></ul><h3 id=updating-owncloud-in-kubernetes>Updating ownCloud in Kubernetes</h3><p>It is the same procedure as with running ownCloud with, e.g., <code>docker-compose</code>.</p><p>To update ownCloud you need to scale down the Deployment to <code>1</code> (<code>replicas</code>), then update the image, wait for the one single Pod come up again and then scale up the ownCloud Deployment again to, e.g., <code>2</code> or more.</p><h2 id=summary-1>Summary</h2><p>This is the end of the two part series on running ownCloud in Kubernetes.</p><p>Have Fun!</p></article><footer class=post-footer><ul class=post-tags><li><a href=https://edenmal.moe/tags/Articles><span class=tag>Articles</span></a></li><li><a href=https://edenmal.moe/tags/ownCloud><span class=tag>OwnCloud</span></a></li></ul><p class=post-copyright>© This page/post is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License, please give source if you wish to quote or reproduce.This post was published <strong>743</strong> days ago, content in the post may be inaccurate, even wrong now, please take risk yourself.</p></footer><script src=https://utteranc.es/client.js repo=galexrt/edenmal.moe issue-term=title label=blogpost theme=icy-dark crossorigin=anonymous async></script></section><footer class=site-footer><p>© 2017-2024 Alexander Trost</p><p>Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> with customized theme <a href=https://github.com/laozhu/hugo-nuo target=_blank rel=noopener>Nuo</a>.</p></footer><link rel=stylesheet href=https://edenmal.moe/styles/icofont.min.min.css><script async src=https://edenmal.moe/cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.min.120e30ad299b8d6548dd1fbb6ab1d45fb508bf080219df63e5ab9750b1241207.js integrity="sha256-Eg4wrSmbjWVI3R+7arHUX7UIvwgCGd9j5auXULEkEgc="></script><script type=text/x-mathjax-config>
  MathJax.Ajax.config.path["MathJax"] = "/cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5";
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      displayMath: [['$$','$$'], ['\\[','\\]']],
      processEscapes: true,
      processEnvironments: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      TeX: { equationNumbers: { autoNumber: "AMS" },
      extensions: ["AMSmath.js", "AMSsymbols.js"] }
    },
  });
</script><script type=text/x-mathjax-config>
  // Fix <code> tags after MathJax finishes running. This is a
  // hack to overcome a shortcoming of Markdown. Discussion at
  // https://github.com/mojombo/jekyll/issues/199
  MathJax.Hub.Queue(() => {
    MathJax.Hub.getAllJax().map(v => v.SourceElement().parentNode.className += ' has-jax');
  });
</script><script src=https://edenmal.moe/scripts/index.min.0375b6e63e18876ff8d4ad95bf8ef081176b6b994398d5bd22f5140f45565d37.js integrity="sha256-A3W25j4Yh2/41K2Vv47wgRdra5lDmNW9IvUUD0VWXTc="></script><script>"serviceWorker"in navigator&&navigator.serviceWorker.register("/service-worker.js").then(function(){console.log("[ServiceWorker] Registered")})</script></body></html>