<!doctype html><html lang=en-us><head><meta charset=utf-8><title>Ceph Day Berlin 2018 | Edenmal - Sysadmin Garden of Eden</title>
<meta name=author content="Alexander Trost"><meta name=description content="Talks, thoughts and pictures from the Ceph Day Berlin 2018."><meta name=twitter:site content="@galexrt"><meta name=twitter:title content="Ceph Day Berlin 2018 | Edenmal - Sysadmin Garden of Eden"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://edenmal.moe/post/2018/Ceph-Day-Berlin-2018/IMG_20181112_091637.jpg"><meta name=twitter:description content="Talks, thoughts and pictures from the Ceph Day Berlin 2018."><meta property="og:type" content="article"><meta property="og:title" content="Ceph Day Berlin 2018"><meta property="og:description" content="Talks, thoughts and pictures from the Ceph Day Berlin 2018."><meta property="og:url" content="https://edenmal.moe/post/2018/Ceph-Day-Berlin-2018/"><meta property="og:image" content="https://edenmal.moe/post/2018/Ceph-Day-Berlin-2018/IMG_20181112_091637.jpg"><meta name=generator content="Hugo 0.120.4"><link rel=canonical href=https://edenmal.moe/post/2018/Ceph-Day-Berlin-2018/><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=format-detection content="telephone=no,email=no,adress=no"><meta http-equiv=Cache-Control content="no-transform"><meta name=robots content="index,follow"><meta name=referrer content="origin-when-cross-origin"><meta name=theme-color content="#4d4ac7"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="black"><meta name=apple-mobile-web-app-title content="Edenmal"><meta name=msapplication-tooltip content="Edenmal"><meta name=msapplication-navbutton-color content="#4d4ac7"><meta name=msapplication-TileColor content="#4d4ac7"><meta name=msapplication-TileImage content="/icons/icon-144x144.png"><link rel=icon href=https://edenmal.moe/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://edenmal.moe/icons/icon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://edenmal.moe/icons/icon-32x32.png><link rel=icon sizes=192x192 href=https://edenmal.moe/icons/icon-192x192.png><link rel=apple-touch-icon href=https://edenmal.moe/icons/icon-152x152.png><link rel=manifest href=https://edenmal.moe/manifest.json><link rel=preload href=https://edenmal.moe/styles/main-rendered.min.css as=style><link rel=preload href=https://edenmal.moe/images/avatar.png as=image><link rel=preload href=https://edenmal.moe/images/grey-prism.svg as=image><style>body{background-color:#f4f3f1;background-image:url(/images/grey-prism.svg);background-repeat:repeat;background-attachment:fixed}</style><link rel=stylesheet href=https://edenmal.moe/styles/main-rendered.min.css><link rel=stylesheet href=https://edenmal.moe/cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe.min.min.css><link rel=stylesheet href=https://edenmal.moe/cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/default-skin/default-skin.min.min.css><script src=https://edenmal.moe/cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe.min.min.819c76b4bba0f8b8c568f6e4860173f1d16e5046c01b008f4705ae823b5766e1.js integrity="sha256-gZx2tLug+LjFaPbkhgFz8dFuUEbAGwCPRwWugjtXZuE="></script><script src=https://edenmal.moe/cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe-ui-default.min.min.2fcfc43be15566e9378af0e8247ba57fec8d4ee95c5dba15cf97e84d6c2115bc.js integrity="sha256-L8/EO+FVZuk3ivDoJHulf+yNTulcXboVz5foTWwhFbw="></script><script src=https://edenmal.moe/cdnjs.cloudflare.com/ajax/libs/jquery/1.12.4/jquery.min.min.77ec5e7d7f844e9b6c2413a352b48194493a65c75d94d5cd9679a225c2da773c.js integrity="sha256-d+xefX+ETptsJBOjUrSBlEk6ZcddlNXNlnmiJcLadzw="></script><script src=https://edenmal.moe/scripts/pswp-init.min.299a530b67d8be41667aa9ed78920e272e970b0ac8ab7540ba3a7c1382b52de2.js integrity="sha256-KZpTC2fYvkFmeqnteJIOJy6XCwrIq3VAujp8E4K1LeI="></script></head><body><div class=suspension><a role=button aria-label="Go to top" title="Go to top" class="to-top is-hide"><span class=icofont-caret-up aria-hidden=true></span></a>
<a role=button aria-label="Print page" title="Print page" class=print-page onclick=window.print()><span class=icofont-print aria-hidden=true></span></a></div><header class=site-header><a class=avatara href=https://edenmal.moe/><img class=avatar src=https://edenmal.moe/images/avatar.png alt=Avatar></a><h2 class=title><a href=https://edenmal.moe/>Edenmal</a></h2><p class=subtitle>Sysadmin Garden of Eden</p><button class=menu-toggle type=button aria-label="Main Menu" aria-expanded=false tab-index=0>
<span class=icofont-navigation-menu aria-hidden=true></span></button><nav class="site-menu collapsed"><h2 class=offscreen>Main Menu</h2><ul class=menu-list><li class="menu-item
is-active"><a href=https://edenmal.moe/>Home</a></li><li class=menu-item><a href=https://edenmal.moe/about/>About</a></li><li class=menu-item><a href=https://edenmal.moe/tags/>Tags</a></li><li class=menu-item><a href=https://edenmal.moe/events/>Events</a></li><li class=menu-item><a href=https://edenmal.moe/site-notice/>Impressum</a></li></ul></nav><nav class="social-menu collapsed"><h2 class=offscreen>Social Networks</h2><ul class=social-list><li class=social-item><a href=mailto:me@galexrt.moe title=Email aria-label=Email><span class=icofont-envelope aria-hidden=true></span></a></li><li class=social-item><a href=https://galexrt.moe/ rel=me title=Homepage aria-label=Homepage><span class=icofont-page aria-hidden=true></span></a></li><li class=social-item><a href=//github.com/galexrt rel=me title=GitHub aria-label=GitHub><span class=icofont-github aria-hidden=true></span></a></li><li class=social-item><a href=//twitter.com/galexrt rel=me title=Twitter aria-label=Twitter><span class=icofont-twitter aria-hidden=true></span></a></li><li class=social-item><a href=//www.linkedin.com/in/alexander-trost rel=me title=LinkedIn aria-label=LinkedIn><span class=icofont-linkedin aria-hidden=true></span></a></li><li class=social-item><a href=//www.xing.com/profile/Alexander_Trost18 rel=me title=XING aria-label=XING><span class=icofont-xing aria-hidden=true></span></a></li><li class=social-item><a rel=alternate type=application/rss+xml href=https://edenmal.moe/index.xml title=RSS aria-label=RSS><span class=icofont-rss aria-hidden=true></span></a></li></ul></nav></header><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><section class="main post-detail"><header class=post-header><h1 class=post-title>Ceph Day Berlin 2018</h1><p class=post-meta>Author Alexander Trost · Read Time 17 minutes · Created Mon Nov 12 08:00:53 2018 · Updated Fri Mar 25 21:33:36 2022</p></header><article class=post-content><div class=fullwidthimg><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=https://edenmal.moe/post/2018/Ceph-Day-Berlin-2018/IMG_20181112_091637.jpg itemprop=contentUrl><img itemprop=thumbnail src=https://edenmal.moe/post/2018/Ceph-Day-Berlin-2018/IMG_20181112_091637.jpg width=850px></a></figure></div><h2>Table of Contents</h2><nav id=TableOfContents><ul><li><a href=#slides>Slides</a></li><li><a href=#welcome--kickoff>Welcome & Kickoff</a></li><li><a href=#talks>Talks</a><ul><li><a href=#state-of-ceph-sage-weil-red-hat>State Of Ceph, Sage Weil, Red Hat</a></li><li><a href=#managing-and-monitoring-ceph-with-the-ceph-manager-dashboard-lenz-grimmer-suse>Managing and Monitoring Ceph with the Ceph Manager Dashboard, Lenz Grimmer, SUSE</a></li><li><a href=#building-a-ceph-storage-appliance-thats-cooler-than-a-dog-phil-straw-softiron>Building a Ceph Storage Appliance That&rsquo;s Cooler Than a Dog, Phil Straw, SoftIron</a></li><li><a href=#unlimited-fileserver-with-samba-ctdb-and-cephfs-robert-sander-heinlein-support>Unlimited Fileserver with Samba CTDB and CephFS, Robert Sander, Heinlein Support</a></li><li><a href=#ceph-implementations-for-the-meerkat-radio-telescope-bennett-sarao-ska-africa>Ceph implementations for the MeerKAT radio telescope, Bennett SARAO, SKA Africa</a></li><li><a href=#disk-health-prediction-and-resource-allocation-for-ceph-by-using-machine-learning-jeremy-wei-prophetstor>Disk health prediction and resource allocation for Ceph by using machine learning, Jeremy Wei, Prophetstor</a></li><li><a href=#mastering-ceph-operations-upmap-and-the-mgr-balancer-dan-van-der-ster-cern>Mastering Ceph Operations: Upmap and the Mgr Balancer, Dan van der Ster, CERN</a></li><li><a href=#deploying-ceph-in-kubernetes-with-rook-sebastian-wagner-suse>Deploying Ceph in Kubernetes with Rook, Sebastian Wagner, SUSE</a></li><li><a href=#ceph-management-the-easy-and-reliable-way-martin-verges-croit>Ceph management the easy and reliable way, Martin Verges, croit</a></li><li><a href=#5-reasons-to-use-arm-based-micro-server-architecture-for-ceph-storage-aaron-joue-ambedded-technology>5 reasons to use Arm-based micro-server architecture for Ceph Storage, Aaron Joue, Ambedded Technology</a></li><li><a href=#practical-cephfs-with-nfs-today-using-openstack-manila-tom-barron-red-hat>Practical CephFS with NFS today using OpenStack Manila, Tom Barron, Red Hat</a></li><li><a href=#ceph-on-the-brain-a-year-with-the-human-brain-project-stig-telfer-stackhpc>Ceph on the Brain: A Year with the Human Brain Project, Stig Telfer, StackHPC</a></li><li><a href=#into-the-cold-object-storage-in-switchengines-simon-leinen-switch>Into the cold: Object Storage in SWITCHengines, Simon Leinen, SWITCH</a></li><li><a href=#qa>Q&amp;A</a></li></ul></li><li><a href=#networking-reception>Networking Reception</a></li><li><a href=#summary>Summary</a></li></ul></nav><hr><blockquote><p><strong>NOTE</strong></p><p>All credit for the slides in the pictures goes to their creators!</p><p><strong>NOTE</strong></p><p>If you are in one of these pictures and want it removed, please contact me by email (see about/imprint page).</p></blockquote><h2 id=slides>Slides</h2><p>The slides of the talks can be found here: <a href=https://www.slideshare.net/Inktank_Ceph>Ceph Community presentations channel - SlideShare</a>.</p><h2 id=welcome--kickoff>Welcome & Kickoff</h2><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_083629.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_083629.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Cloudical team arrived at the conference center</h4></figcaption></figure><p>I arrived with my colleagues from <a href=https://cloudical.io/>Cloudical</a> at the Ceph Day Berlin location, which at the same time was the location for <a href=https://www.openstack.org/summit/berlin-2018/>OpenStack Summit</a>.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_091336.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_091336.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Ceph Day Sponsors</h4></figcaption></figure><p><a href=https://twitter.com/liewegas>Sage Weil</a> announced that the Ceph Foundation has been founded with help from the Linux Foundation.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_091637.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_091637.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Ceph Foundation</h4></figcaption></figure><p>Cephalocon Barcelona has been announced. It will take place just &ldquo;before&rdquo; KubeCon and just shows that the Ceph team wants to be storage for cloud and containers.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_092128.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_092128.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Cephalocon Barcelona announcement</h4></figcaption></figure><h2 id=talks>Talks</h2><h3 id=state-of-ceph-sage-weil-red-hat>State Of Ceph, Sage Weil, Red Hat</h3><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_092242.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_092242.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Ceph Release Schedule Overview</h4></figcaption></figure><p>There will be a telemetry module for the Ceph MGR but it will be opt-in. The telemetry data will be anonymized and only &ldquo;basic&rdquo; stats such as OSD count, cluster size and so on are transmitted.</p><p>To help Ceph clusters with their reliability, features have been added to take the health of devices/disks into account. Disk failure prediction can be used to preemptively move data to other disks/servers before a failure to reduce recovery time and cluster load.</p><p>The Ceph MGR in general and the Ceph MGR dashboard module have been further improved by <a href=https://github.com/jcsp>John Spray</a>.
Additionally to the Ceph MGR becoming more and more stable, the REST API module for the MGR has been improved. The REST API isn&rsquo;t completely &ldquo;there&rdquo; yet, but it is actively being worked on.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_092350.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_092350.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - State Of Ceph, Sage Weil, Red Hat - Automation and Management</h4></figcaption></figure><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_092726.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_092726.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - State Of Ceph, Sage Weil, Red Hat - Orchestrator Sandwhich</h4></figcaption></figure><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_092913.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_092913.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - State Of Ceph, Sage Weil, Red Hat - Kubernetes Above and Below</h4></figcaption></figure><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_093016.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_093016.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - State Of Ceph, Sage Weil, Red Hat - Rook</h4></figcaption></figure><p>The goal for Ceph in Kubernetes is to further go the cloud-native way by making it more and more easy to have storage for containers.</p><p><code>ceph top</code> is like <code>top</code> but for Ceph cluster. It will show you the current &ldquo;load&rdquo;/usage of the whole Ceph cluster.</p><p><code>cephfs-shell</code> is a command cli which allows to work with the CephFS without &ldquo;mounting&rdquo; it. That is perfect for quickly putting files, directories and setting quotas on a CephFS through scripts/automation.</p><p>Project Crimson is looking into re-implementing the OSD data path for better performance, with a focus on &ldquo;new&rdquo; devices, e.g., flash and NVME storage.</p><p>For new modern IT infrastructures, it is planned to &ldquo;Reframe Ceph RGW&rdquo;. This is done to make it more suitable for modern IT infrastructures which most of the time rely on multiple cloud providers/datacenter.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_093803.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_093803.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - State Of Ceph, Sage Weil, Red Hat - Hardware is changing</h4></figcaption></figure><p>RGW federation/sync can allow you to have global user and bucket namespaces in sync over multiple Ceph RGW clusters/zones.
RGW sync (now) available to e.g., sync an on-premise Ceph RGW cluster with an AWS S3 for &ldquo;backup&rdquo; or just keep two (or more) Ceph RGW clusters in sync.</p><h3 id=managing-and-monitoring-ceph-with-the-ceph-manager-dashboard-lenz-grimmer-suse>Managing and Monitoring Ceph with the Ceph Manager Dashboard, Lenz Grimmer, SUSE</h3><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_094522.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_094522.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Managing and Monitoring Ceph with the Ceph Manager Dashboard, Lenz Grimmer, SUSE - Title Slide</h4></figcaption></figure><blockquote><p><strong>NOTE</strong> He has a big Rook sticker on his laptop, which is totally cool as they are currently nowhere available anymore. :-)</p></blockquote><p>They mentioned at last <a href=https://ceph.com/cephdays/germany/>Ceph Day Germany in Darmstadt</a> that the dashboard was available now and is hugely inspired by the <a href=https://www.openattic.org/>OpenATTIC</a> project.
So thanks for them to put the work into the official Ceph MGR dashboard module!</p><p>They have more features coming up for the Ceph Nautilus release. A big part will be to allow even further configuration over the web with ease.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_095006.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_095006.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Managing and Monitoring Ceph with the Ceph Manager Dashboard, Lenz Grimmer, SUSE - Dashboard v2 Overview (Mimic)</h4></figcaption></figure><p>I&rsquo;m especially happy to see the in-built SAML2 support and the auditing support which will be useful for companies.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_095333.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_095333.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Managing and Monitoring Ceph with the Ceph Manager Dashboard, Lenz Grimmer, SUSE - New Dashboard features for Nautilus</h4></figcaption></figure><p>You can also have a per server overview page with metrics displayed through Grafana.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_095702.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_095702.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Managing and Monitoring Ceph with the Ceph Manager Dashboard, Lenz Grimmer, SUSE - Dashboard Screenshot #1</h4></figcaption></figure><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_095900.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_095900.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Managing and Monitoring Ceph with the Ceph Manager Dashboard, Lenz Grimmer, SUSE - Dashboard Screenshot #2</h4></figcaption></figure><p>Next to &ldquo;just being a dashboard&rdquo;, as mentioned earlier there is a focus on allowing a user to make changes to the Ceph config through the Ceph MGR dashboard.
Creating snapshots, clones of <code>rbd</code> images is now also possible through the dashboard.</p><p>Even if it is a small thing, thanks to a user contribution You can now see as which user you are logged into the dashboard now.</p><p>To summarize, the dashboard can not only show but can also create, modify and also delete &ldquo;things&rdquo; in Ceph and edit configurations.</p><p>An outlook for the dashboard is to integrate the upcoming orchestration layer of the Ceph MGR.</p><h3 id=building-a-ceph-storage-appliance-thats-cooler-than-a-dog-phil-straw-softiron>Building a Ceph Storage Appliance That&rsquo;s Cooler Than a Dog, Phil Straw, SoftIron</h3><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_104610.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_104610.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Building a Ceph Storage Appliance That's Cooler Than a Dog, Phil Straw, SoftIron - Title Slide</h4></figcaption></figure><p>SoftIron does a lot of thermal analysis of servers.</p><p>When they got their new thermal camera, next to taking pictures of dogs with it, they took pictures of their competitors and their own servers.
They do that to see their server cooling optimization against the competition:</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_104844.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_104844.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Building a Ceph Storage Appliance That's Cooler Than a Dog, Phil Straw, SoftIron - 'And the second thing?' - Using thermal camera on servers</h4></figcaption></figure><blockquote><p>&ldquo;Power is heat&rdquo;, Power costs money, cooling also costs money = A lot of money</p></blockquote><p>If the design of the servers is optimized for &ldquo;being cool&rdquo;, one can potentially get back parts/all of their costs of the equipment back through saving on cooling costs.
Though in most cases this would require separate datacenter rooms for the optimized servers with less cooling.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_105034.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_105034.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Building a Ceph Storage Appliance That's Cooler Than a Dog, Phil Straw, SoftIron - 'Thermal Analysis' server comparison</h4></figcaption></figure><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_105155.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_105155.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Building a Ceph Storage Appliance That's Cooler Than a Dog, Phil Straw, SoftIron - 'Derivation of "Dog Power"- Rest'</h4></figcaption></figure><p>A round of applause for their dog.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_105952.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_105952.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Building a Ceph Storage Appliance That's Cooler Than a Dog, Phil Straw, SoftIron - 'How do we do it?'</h4></figcaption></figure><p>In the end their goal is to have &ldquo;cool&rdquo; servers which can &ldquo;quickly&rdquo; get back the costs through the servers reduced cooling needs.</p><h3 id=unlimited-fileserver-with-samba-ctdb-and-cephfs-robert-sander-heinlein-support>Unlimited Fileserver with Samba CTDB and CephFS, Robert Sander, Heinlein Support</h3><blockquote><p><strong>NOTE</strong></p><p>As this isn&rsquo;t really a topic which falls into my interest area. I&rsquo;d recommend to checkout the full slides of the talk on the <a href=#slides>SlideShare</a>. Now have a few pictures without too much comments from my side:</p></blockquote><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_110331.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_110331.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Unlimited Fileserver with Samba CTDB and CephFS, Robert Sander, Heinlein Support - 'Concept'</h4></figcaption></figure><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_110847.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_110847.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Unlimited Fileserver with Samba CTDB and CephFS, Robert Sander, Heinlein Support - 'CTDB - clustered trivial database'</h4></figcaption></figure><h3 id=ceph-implementations-for-the-meerkat-radio-telescope-bennett-sarao-ska-africa>Ceph implementations for the MeerKAT radio telescope, Bennett SARAO, SKA Africa</h3><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_112547.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_112547.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Ceph implementations for the MeerKAT radio telescope, Bennett SARAO, SKA Africa - Title Slide</h4></figcaption></figure><p>After some technical difficulties, he started talking about their work with Ceph at <a href=https://www.skatelescope.org/>SKA Africa</a>.</p><blockquote><p>&ldquo;All radio telescopes have received less energy than a snowflake hitting the ground.&rdquo;</p></blockquote><p>This comment just makes it more and more interesting of how sensitive the radio dishes must be and with that how much data is coming from the sensors.</p><p>With more radio dishes the quality of pictures generated from the observations increases, resulting in a picture of the &ldquo;sky&rdquo;/universe which is more and more clear.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_113048.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_113048.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Ceph implementations for the MeerKAT radio telescope, Bennett SARAO, SKA Africa - 'A fuller picture'</h4></figcaption></figure><p>All these radio dishes create an enormous amount of data which needs to be saved, this is where Ceph comes into play.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_113927.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_113927.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Ceph implementations for the MeerKAT radio telescope, Bennett SARAO, SKA Africa - 'Radio Telescopes - In a nutshell'</h4></figcaption></figure><p>They have tons of data flowing in from the radios dishes which doesn&rsquo;t only need to be written but also read again and worked on in a timely manner.
A problem they had to solve is the data transfer between the radio dishes and the datacenter, which is over 900 kilometers away.
They shielded their data lines so that it &ldquo;survives&rdquo; the 900 kilometer transfer.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_114031.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_114031.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Ceph implementations for the MeerKAT radio telescope, Bennett SARAO, SKA Africa - 'SDP Data Rates'</h4></figcaption></figure><p>What I didn&rsquo;t think is that they use Ceph RGW S3 storage. It just shows how good object storage can be for such amounts of data in their use case.</p><p>As they had project budget problems, because of the &ldquo;decline&rdquo; of the African dollar in comparison to the US dollar, they used available government programs for such technological projects which brought them together with local IT companies to save costs for the hardware used at the project.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_114447.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_114447.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Ceph implementations for the MeerKAT radio telescope, Bennett SARAO, SKA Africa - 'Ceph - MeerKAT cluster to SeeKAT cluster'</h4></figcaption></figure><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_114839.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_114839.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Ceph implementations for the MeerKAT radio telescope, Bennett SARAO, SKA Africa - 'Ceph Hardware Platform'</h4></figcaption></figure><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_115345_1.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_115345_1.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Ceph implementations for the MeerKAT radio telescope, Bennett SARAO, SKA Africa - '16 Node Ceph Cluster - Production until end November 2018'</h4></figcaption></figure><p>Interesting insights were given in the talk in how Ceph is used for such amounts from data for scientific research.</p><h3 id=disk-health-prediction-and-resource-allocation-for-ceph-by-using-machine-learning-jeremy-wei-prophetstor>Disk health prediction and resource allocation for Ceph by using machine learning, Jeremy Wei, Prophetstor</h3><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_115752.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_115752.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Disk health prediction and resource allocation for Ceph by using machine learning, Jeremy Wei, Prophetstor - 'Major Stability problem: Ceph Cluster'</h4></figcaption></figure><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_120004.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_120004.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Disk health prediction and resource allocation for Ceph by using machine learning, Jeremy Wei, Prophetstor - 'Ceph DiskPrediction Plugin'</h4></figcaption></figure><p>The disk health prediction is/will be merged into Ceph Mimic to allow for smarter data placement and/or preemptive movement of data before a disk fails.
There will also be a mode for cloud providers to read the health of used cloud provider storage volume(s) and to act accordingly before there would be a failure on the cloud provider site in point of performance and/or latency hits.</p><p><a href=https://www.prophetstor.com>Prophetstor</a> will offer a commercial edition which has further support for even better detection and especially interesting anomaly detection for disks.</p><blockquote><p><strong>NOTE</strong></p><p><a href=https://github.com/containers-ai/Alameda>Alameda</a> is the open source project for such resource predictions and failure detection. A <a href=https://github.com/rook/rook/pull/2182>pull request</a> is open at the Rook.io project for a design document.</p></blockquote><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_120622.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_120622.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Disk health prediction and resource allocation for Ceph by using machine learning, Jeremy Wei, Prophetstor - 'The Brain of Resources Orchestrator for Kubernetes'</h4></figcaption></figure><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_120740.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_120740.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Disk health prediction and resource allocation for Ceph by using machine learning, Jeremy Wei, Prophetstor - 'Pod Allocation Before/After Alameda Enabled'</h4></figcaption></figure><p>Utilizing Kubernetes seems like the perfect way here as the workload in the Kubernetes cluster can be automatically adjusted/moved depending on the predicted information.</p><h3 id=mastering-ceph-operations-upmap-and-the-mgr-balancer-dan-van-der-ster-cern>Mastering Ceph Operations: Upmap and the Mgr Balancer, Dan van der Ster, CERN</h3><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_130923.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_130923.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Mastering Ceph Operations: Upmap and the Mgr Balancer, Dan van der Ster, CERN - 'CephFS Scale Testing'</h4></figcaption></figure><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_131310.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_131310.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Mastering Ceph Operations: Upmap and the Mgr Balancer, Dan van der Ster, CERN - 'Do we really need CRUSH?'</h4></figcaption></figure><p>In Luminous there is a new feature named <a href=http://docs.ceph.com/docs/mimic/rados/operations/upmap/><code>upmap</code></a> and a <a href=http://docs.ceph.com/docs/mimic/mgr/balancer/>PG balancer</a>.</p><ul><li><a href=http://docs.ceph.com/docs/mimic/rados/operations/upmap/><code>upmap</code></a> - Allows you to move PGs exactly where you want them.</li><li><a href=http://docs.ceph.com/docs/mimic/mgr/balancer/>PG Balancer</a> - Can automatically balance PGs between servers.</li></ul><p>The PG balancer allowed CERN to recover terrabytes of data because of how PGs were automaticallly rebalanced between OSDs.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_131659.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_131659.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Mastering Ceph Operations: Upmap and the Mgr Balancer, Dan van der Ster, CERN - 'Turning on the balancer (luminous)'</h4></figcaption></figure><p>(You need to restart the MGR (in Luminous) when this configuration has been set)</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_131855.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_131855.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Mastering Ceph Operations: Upmap and the Mgr Balancer, Dan van der Ster, CERN - 'A brief interlude...'</h4></figcaption></figure><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_132154.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_132154.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Mastering Ceph Operations: Upmap and the Mgr Balancer, Dan van der Ster, CERN - 'Adding capacity with upmap'</h4></figcaption></figure><p><code>upmap</code> allows you to not worry about changes that would affect your PGs in point of being moved and/or rebalanced. Using <code>upmap</code> would basically allow you &ldquo;pin&rdquo;/map PGs to &ldquo;stay&rdquo; where they are or &ldquo;force&rdquo; them to somewhere else.
This is extremely useful if you change placement options which would normally require (for big amounts of data) weeks long of PG rebalancing. In such a case you would just <code>upmap</code> the PGs and they stay where they are when you made your placement changes.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_132720.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_132720.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Mastering Ceph Operations: Upmap and the Mgr Balancer, Dan van der Ster, CERN - 'No leap of faith required'</h4></figcaption></figure><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_132725.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_132725.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Mastering Ceph Operations: Upmap and the Mgr Balancer, Dan van der Ster, CERN - 'What's next for "upmap remapped"'</h4></figcaption></figure><p>I think this would make a great core feature for Ceph. Though I think it would need to see improvements in user experience as it seems a bit complicated without as he said &ldquo;their scripts&rdquo; which generated most of their commands.</p><h3 id=deploying-ceph-in-kubernetes-with-rook-sebastian-wagner-suse>Deploying Ceph in Kubernetes with Rook, Sebastian Wagner, SUSE</h3><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_133144.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_133144.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Deploying Ceph in Kubernetes with Rook, Sebastian Wagner, SUSE - Title Slide</h4></figcaption></figure><p>It is good to see SUSE and RedHat talk about Rook.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_134118.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_134118.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Deploying Ceph in Kubernetes with Rook, Sebastian Wagner, SUSE - 'Integration into Ceph'</h4></figcaption></figure><p>A big point which is partly going to be &ldquo;solved&rdquo; by the MGR orchestration layer/module, that the orchestration &ldquo;systems&rdquo;, e.g., Rook, Ansible, DeepSEA and others, can simply integrate/implement the &ldquo;bridge&rdquo; to be able to be used as a orchestration &ldquo;helper&rdquo;.
In the end, it shouldn&rsquo;t matter if you want to run Ceph using Kubernetes, on bare metal using Ansible or DeepSEA and other possible environments. The Ceph MGR is currently, with the orchestration layer/module targeted to be the interface to do that.</p><p>If you want information about Rook, I would recommend to checkout the Rook project homepage: <a href=https://rook.io/>https://rook.io/</a>.</p><h3 id=ceph-management-the-easy-and-reliable-way-martin-verges-croit>Ceph management the easy and reliable way, Martin Verges, croit</h3><p>Their approach boots the OS in an in-ram overlay filesystem for simple and easy updating. In addition to that, new servers register themselves by a &ldquo;config&rdquo; given to them by the PXE server.</p><p>When the PXE server is setup each node just needs to PXE boot, then one can select the nodes and disks on the nodes, and click the &ldquo;Create&rdquo; button in their webinterface to manage the node(s). <code>ceph-volume</code> will then be used to provision the disks for use as OSDs in Ceph.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_135442.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_135442.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Ceph management the easy and reliable way, Martin Verges, croit - 'Is it Possible to Beat the Prices of Amazon Glacier with Ceph?'</h4></figcaption></figure><p>There is a Vagrant demo available for <a href=https://croit.io/>croit</a> in action, see <a href=https://github.com/croit/vagrant-demo>GitHub croit/vagrant-demo</a>.</p><h3 id=5-reasons-to-use-arm-based-micro-server-architecture-for-ceph-storage-aaron-joue-ambedded-technology>5 reasons to use Arm-based micro-server architecture for Ceph Storage, Aaron Joue, Ambedded Technology</h3><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_143157.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_143157.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - 5 reasons to use Arm-based micro-server architecture for Ceph Storage, Aaron Joue, Ambedded Technology - 'Ceph is Scalable & No Single Point of Failure'</h4></figcaption></figure><blockquote><p>&ldquo;Even with Ceph, hardware failure is painful.&rdquo;</p></blockquote><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_143730.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_143730.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - 5 reasons to use Arm-based micro-server architecture for Ceph Storage, Aaron Joue, Ambedded Technology - '(1) Minimize the Failure Domain'</h4></figcaption></figure><p>Using ARM based servers in a &ldquo;certain way&rdquo; allows to have the same performance at lower server and power costs.</p><p>Due to the servers using less power, you end up with a higher density than with most &ldquo;normal&rdquo; servers.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_143845.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_143845.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - 5 reasons to use Arm-based micro-server architecture for Ceph Storage, Aaron Joue, Ambedded Technology - 'Arm-based Micro-Server Architecture'</h4></figcaption></figure><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_144155.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_144155.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - 5 reasons to use Arm-based micro-server architecture for Ceph Storage, Aaron Joue, Ambedded Technology - '(3) Bonus: Reduce Power Consumprtion'</h4></figcaption></figure><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_144653.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_144653.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - 5 reasons to use Arm-based micro-server architecture for Ceph Storage, Aaron Joue, Ambedded Technology - 'Object Store Performance'</h4></figcaption></figure><p>It is good to see performance benchmarks for Arm64 servers.</p><h3 id=practical-cephfs-with-nfs-today-using-openstack-manila-tom-barron-red-hat>Practical CephFS with NFS today using OpenStack Manila, Tom Barron, Red Hat</h3><blockquote><p><strong>NOTE</strong></p><p>As this isn&rsquo;t really a topic which falls into my interest area. I&rsquo;d recommend to checkout the full slides of the talk on the <a href=#slides>SlideShare</a>. Now have a few pictures without too much comments from my side:</p></blockquote><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_150046.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_150046.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Practical CephFS with NFS today using OpenStack Manila, Tom Barron, Red Hat - 'There's a perfectly good native CephFS solution for Manila'</h4></figcaption></figure><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_150209.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_150209.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Practical CephFS with NFS today using OpenStack Manila, Tom Barron, Red Hat - 'Why NFS Ganesha?'</h4></figcaption></figure><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_150429.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_150429.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Practical CephFS with NFS today using OpenStack Manila, Tom Barron, Red Hat - 'CephFS NFS driver deployment with TripleO'</h4></figcaption></figure><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_151716.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_151716.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Practical CephFS with NFS today using OpenStack Manila, Tom Barron, Red Hat - 'Current CephFS NFS Driver'</h4></figcaption></figure><p>Software mentioned in the presentation slides linked here for your convenience:</p><ul><li><a href=https://www.rdoproject.org/tripleo/>TripleO</a></li><li><a href=https://wiki.openstack.org/wiki/Manila>Manila</a></li><li><a href=https://github.com/nfs-ganesha/nfs-ganesha/wiki>NFS Ganesha</a></li></ul><h3 id=ceph-on-the-brain-a-year-with-the-human-brain-project-stig-telfer-stackhpc>Ceph on the Brain: A Year with the Human Brain Project, Stig Telfer, StackHPC</h3><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_152140.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_152140.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Ceph on the Brain: A Year with the Human Brain Project, Stig Telfer, StackHPC - 'Ceph on the Brain!'</h4></figcaption></figure><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_152446.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_152446.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Ceph on the Brain: A Year with the Human Brain Project, Stig Telfer, StackHPC - 'JULIA pilot system'</h4></figcaption></figure><blockquote><p>&ldquo;Hot hardware when it was 2016&rdquo;</p></blockquote><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_152700.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_152700.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Ceph on the Brain: A Year with the Human Brain Project, Stig Telfer, StackHPC - 'Ceph's Performance Record'</h4></figcaption></figure><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_152852.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_152852.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Ceph on the Brain: A Year with the Human Brain Project, Stig Telfer, StackHPC - 'JULIA Cluster Fabric'</h4></figcaption></figure><p>It is amazing to see that they reach around 5GBits on bluestore instead of mere 400-500 Mbit/s on filestore.</p><p>Interestingly they had immense losses in performance (from around 16 GBit/s to 4-5 Gbit/s) when they tried using LVM for the disks instead of direct partitions.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_153501.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_153501.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Ceph on the Brain: A Year with the Human Brain Project, Stig Telfer, StackHPC - 'Write Amplification - Bluestore'</h4></figcaption></figure><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_153934.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_153934.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Ceph on the Brain: A Year with the Human Brain Project, Stig Telfer, StackHPC - 'Spectre/Meltdown Mitigations'</h4></figcaption></figure><p>Those loses of performance because of Spectre/Meltdown Mittigations&mldr; <strong>15.5%</strong> I/O performance loss is huge in &ldquo;any&rdquo; Ceph environment. Not only looking at Ceph, for any application such a loss of performance is problem.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_154358.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_154358.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Ceph on the Brain: A Year with the Human Brain Project, Stig Telfer, StackHPC - 'Burst Buffer Workflows'</h4></figcaption></figure><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_154625.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_154625.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Ceph on the Brain: A Year with the Human Brain Project, Stig Telfer, StackHPC - 'New Developments in Ceph-RDMA'</h4></figcaption></figure><h3 id=into-the-cold-object-storage-in-switchengines-simon-leinen-switch>Into the cold: Object Storage in SWITCHengines, Simon Leinen, SWITCH</h3><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_161153.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_161153.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Into the cold: Object Storage in SWITCHengines, Simon Leinen, SWITCH - 'Users Discover Object Storage: SWITCHtube'</h4></figcaption></figure><blockquote><p>&ldquo;Like YouTube but without ads&rdquo; most likely only for educational videos.</p></blockquote><p>They use object storage not only for &ldquo;cat pictures/videos and etc&rdquo; but also for research data and everything else.</p><p>Keystone is &ldquo;Denial of Service"d when it is not tweaked to &ldquo;perfection&rdquo; to handle many requests, causing increase in latency for every request to the object storage.</p><p>It is a challenge to (cost efficiently) save data for long-term.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_162629.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_162629.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Into the cold: Object Storage in SWITCHengines, Simon Leinen, SWITCH - 'Customer Requirements/Expectations'</h4></figcaption></figure><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_162701.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_162701.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Into the cold: Object Storage in SWITCHengines, Simon Leinen, SWITCH - 'Cost/Performance'</h4></figcaption></figure><p>Using (aggressive) erasure coded for long-term storage sounds reasonable as long as the SLAs are correctly communicated and able to be kept with the chosen erasure coded configuration.</p><p>(As long as the aggressive erasure coded profiles don&rsquo;t attack you while administrating the Ceph cluster)</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_163811.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_163811.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Into the cold: Object Storage in SWITCHengines, Simon Leinen, SWITCH - '"Moonshot" challenge for Ceph (or other SDS)'</h4></figcaption></figure><h3 id=qa>Q&amp;A</h3><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_164210.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_164210.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Q&amp;A Session - Speakers on stage</h4></figcaption></figure><p>Cool to see all speakers that were still there, answering questions of the attendees.</p><h2 id=networking-reception>Networking Reception</h2><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=IMG_20181112_160852.jpg itemprop=contentUrl><img itemprop=thumbnail src=IMG_20181112_160852.jpg width=700px></a><figcaption><h4>Ceph Day Berlin 2018 - Networking Reception</h4></figcaption></figure><p>Free beer! After a whole day of talks about Ceph and talking to people about Rook, we were happy and exhausted.</p><p>I hope to see most Ceph Day attendees at one of the next Ceph Day.</p><h2 id=summary>Summary</h2><p>The Ceph foundation is an awesome step forward for Ceph and projects using Ceph like Rook!</p><p>It was cool meeting <a href=https://twitter.com/liewegas>Sage Weil</a> and Sebastian Wagner in person instead of just through Rook meeting calls.
The talks were interesting, though in general for me some were more interesting than others, but it was the perfect mix of topics.</p><p>Have Fun!</p></article><footer class=post-footer><ul class=post-tags><li><a href=https://edenmal.moe/tags/Conferences><span class=tag>Conferences</span></a></li><li><a href=https://edenmal.moe/tags/Ceph><span class=tag>Ceph</span></a></li><li><a href=https://edenmal.moe/tags/Storage><span class=tag>Storage</span></a></li></ul><p class=post-copyright>© This page/post is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License, please give source if you wish to quote or reproduce.This post was published <strong>650</strong> days ago, content in the post may be inaccurate, even wrong now, please take risk yourself.</p></footer><script src=https://utteranc.es/client.js repo=galexrt/edenmal.moe issue-term=title label=blogpost theme=icy-dark crossorigin=anonymous async></script></section><footer class=site-footer><p>© 2017-2024 Alexander Trost</p><p>Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> with customized theme <a href=https://github.com/laozhu/hugo-nuo target=_blank rel=noopener>Nuo</a>.</p></footer><link rel=stylesheet href=https://edenmal.moe/styles/icofont.min.min.css><script async src=https://edenmal.moe/cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.min.120e30ad299b8d6548dd1fbb6ab1d45fb508bf080219df63e5ab9750b1241207.js integrity="sha256-Eg4wrSmbjWVI3R+7arHUX7UIvwgCGd9j5auXULEkEgc="></script><script type=text/x-mathjax-config>
  MathJax.Ajax.config.path["MathJax"] = "/cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5";
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      displayMath: [['$$','$$'], ['\\[','\\]']],
      processEscapes: true,
      processEnvironments: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      TeX: { equationNumbers: { autoNumber: "AMS" },
      extensions: ["AMSmath.js", "AMSsymbols.js"] }
    },
  });
</script><script type=text/x-mathjax-config>
  // Fix <code> tags after MathJax finishes running. This is a
  // hack to overcome a shortcoming of Markdown. Discussion at
  // https://github.com/mojombo/jekyll/issues/199
  MathJax.Hub.Queue(() => {
    MathJax.Hub.getAllJax().map(v => v.SourceElement().parentNode.className += ' has-jax');
  });
</script><script src=https://edenmal.moe/scripts/index.min.0375b6e63e18876ff8d4ad95bf8ef081176b6b994398d5bd22f5140f45565d37.js integrity="sha256-A3W25j4Yh2/41K2Vv47wgRdra5lDmNW9IvUUD0VWXTc="></script><script>"serviceWorker"in navigator&&navigator.serviceWorker.register("/service-worker.js").then(function(){console.log("[ServiceWorker] Registered")})</script></body></html>