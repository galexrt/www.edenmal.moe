<!doctype html><html lang=en-us><head><meta charset=utf-8><title>Container and Kubernetes - Day #3 | Edenmal - Sysadmin Garden of Eden</title>
<meta name=author content="Alexander Trost"><meta name=description content="Container und Kubernetes Training Material"><meta name=twitter:site content="@galexrt"><meta name=twitter:title content="Container and Kubernetes - Day #3 | Edenmal - Sysadmin Garden of Eden"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://edenmal.moe/post/covers/kubernetes-logo-with-bg.png"><meta name=twitter:description content="Container und Kubernetes Training Material"><meta property="og:type" content="article"><meta property="og:title" content="Container and Kubernetes - Day #3"><meta property="og:description" content="Container und Kubernetes Training Material"><meta property="og:url" content="https://edenmal.moe/post/2019/Container-and-Kubernetes-Training-Day3/"><meta property="og:image" content="https://edenmal.moe/post/covers/kubernetes-logo-with-bg.png"><meta name=generator content="Hugo 0.120.4"><link rel=canonical href=https://edenmal.moe/post/2019/Container-and-Kubernetes-Training-Day3/><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=format-detection content="telephone=no,email=no,adress=no"><meta http-equiv=Cache-Control content="no-transform"><meta name=robots content="index,follow"><meta name=referrer content="origin-when-cross-origin"><meta name=theme-color content="#4d4ac7"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="black"><meta name=apple-mobile-web-app-title content="Edenmal"><meta name=msapplication-tooltip content="Edenmal"><meta name=msapplication-navbutton-color content="#4d4ac7"><meta name=msapplication-TileColor content="#4d4ac7"><meta name=msapplication-TileImage content="/icons/icon-144x144.png"><link rel=icon href=https://edenmal.moe/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://edenmal.moe/icons/icon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://edenmal.moe/icons/icon-32x32.png><link rel=icon sizes=192x192 href=https://edenmal.moe/icons/icon-192x192.png><link rel=apple-touch-icon href=https://edenmal.moe/icons/icon-152x152.png><link rel=manifest href=https://edenmal.moe/manifest.json><link rel=preload href=https://edenmal.moe/styles/main-rendered.min.css as=style><link rel=preload href=https://edenmal.moe/images/avatar.png as=image><link rel=preload href=https://edenmal.moe/images/grey-prism.svg as=image><style>body{background-color:#f4f3f1;background-image:url(/images/grey-prism.svg);background-repeat:repeat;background-attachment:fixed}</style><link rel=stylesheet href=https://edenmal.moe/styles/main-rendered.min.css><link rel=stylesheet href=https://edenmal.moe/cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe.min.min.css><link rel=stylesheet href=https://edenmal.moe/cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/default-skin/default-skin.min.min.css><script src=https://edenmal.moe/cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe.min.min.819c76b4bba0f8b8c568f6e4860173f1d16e5046c01b008f4705ae823b5766e1.js integrity="sha256-gZx2tLug+LjFaPbkhgFz8dFuUEbAGwCPRwWugjtXZuE="></script><script src=https://edenmal.moe/cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe-ui-default.min.min.2fcfc43be15566e9378af0e8247ba57fec8d4ee95c5dba15cf97e84d6c2115bc.js integrity="sha256-L8/EO+FVZuk3ivDoJHulf+yNTulcXboVz5foTWwhFbw="></script><script src=https://edenmal.moe/cdnjs.cloudflare.com/ajax/libs/jquery/1.12.4/jquery.min.min.77ec5e7d7f844e9b6c2413a352b48194493a65c75d94d5cd9679a225c2da773c.js integrity="sha256-d+xefX+ETptsJBOjUrSBlEk6ZcddlNXNlnmiJcLadzw="></script><script src=https://edenmal.moe/scripts/pswp-init.min.299a530b67d8be41667aa9ed78920e272e970b0ac8ab7540ba3a7c1382b52de2.js integrity="sha256-KZpTC2fYvkFmeqnteJIOJy6XCwrIq3VAujp8E4K1LeI="></script></head><body><div class=suspension><a role=button aria-label="Go to top" title="Go to top" class="to-top is-hide"><span class=icofont-caret-up aria-hidden=true></span></a>
<a role=button aria-label="Print page" title="Print page" class=print-page onclick=window.print()><span class=icofont-print aria-hidden=true></span></a></div><header class=site-header><a class=avatara href=https://edenmal.moe/><img class=avatar src=https://edenmal.moe/images/avatar.png alt=Avatar></a><h2 class=title><a href=https://edenmal.moe/>Edenmal</a></h2><p class=subtitle>Sysadmin Garden of Eden</p><button class=menu-toggle type=button aria-label="Main Menu" aria-expanded=false tab-index=0>
<span class=icofont-navigation-menu aria-hidden=true></span></button><nav class="site-menu collapsed"><h2 class=offscreen>Main Menu</h2><ul class=menu-list><li class="menu-item
is-active"><a href=https://edenmal.moe/>Home</a></li><li class=menu-item><a href=https://edenmal.moe/about/>About</a></li><li class=menu-item><a href=https://edenmal.moe/tags/>Tags</a></li><li class=menu-item><a href=https://edenmal.moe/events/>Events</a></li><li class=menu-item><a href=https://edenmal.moe/site-notice/>Impressum</a></li></ul></nav><nav class="social-menu collapsed"><h2 class=offscreen>Social Networks</h2><ul class=social-list><li class=social-item><a href=mailto:me@galexrt.moe title=Email aria-label=Email><span class=icofont-envelope aria-hidden=true></span></a></li><li class=social-item><a href=https://galexrt.moe/ rel=me title=Homepage aria-label=Homepage><span class=icofont-page aria-hidden=true></span></a></li><li class=social-item><a href=//github.com/galexrt rel=me title=GitHub aria-label=GitHub><span class=icofont-github aria-hidden=true></span></a></li><li class=social-item><a href=//twitter.com/galexrt rel=me title=Twitter aria-label=Twitter><span class=icofont-twitter aria-hidden=true></span></a></li><li class=social-item><a href=//www.linkedin.com/in/alexander-trost rel=me title=LinkedIn aria-label=LinkedIn><span class=icofont-linkedin aria-hidden=true></span></a></li><li class=social-item><a href=//www.xing.com/profile/Alexander_Trost18 rel=me title=XING aria-label=XING><span class=icofont-xing aria-hidden=true></span></a></li><li class=social-item><a rel=alternate type=application/rss+xml href=https://edenmal.moe/index.xml title=RSS aria-label=RSS><span class=icofont-rss aria-hidden=true></span></a></li></ul></nav></header><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><section class="main post-detail"><header class=post-header><h1 class=post-title>Container and Kubernetes - Day #3</h1><p class=post-meta>Author Alexander Trost · Read Time 14 minutes · Created Tue Mar 26 21:27:06 2019 · Updated Sat Mar 26 17:28:53 2022</p></header><article class=post-content><div class=fullwidthimg><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=https://edenmal.moe/post/covers/kubernetes-logo-with-bg.png itemprop=contentUrl><img itemprop=thumbnail src=https://edenmal.moe/post/covers/kubernetes-logo-with-bg.png width=850px></a></figure></div><h2>Table of Contents</h2><nav id=TableOfContents><ul><li><a href=#welcome>Welcome!</a></li><li><a href=#goal-of-the-training>Goal of the Training</a></li><li><a href=#goal-of-the-day>Goal of the day</a></li><li><a href=#architecture--components>Architecture + Components</a><ul><li><a href=#master-components>Master Components</a></li><li><a href=#node-components>Node Components</a></li><li><a href=#addons>Addons</a></li></ul></li><li><a href=#operating-a-kubernetes-cluster>Operating a Kubernetes Cluster</a><ul><li><a href=#what-is-an-operator-making-your-life-easier>What is an Operator? Making your life easier.</a></li><li><a href=#monitoring>Monitoring</a></li><li><a href=#logging>Logging</a></li><li><a href=#storage>Storage</a></li><li><a href=#maintenance-tasks>Maintenance Tasks</a></li></ul></li><li><a href=#summary-of-the-day>Summary of the Day</a></li></ul></nav><hr><h2 id=welcome>Welcome!</h2><p>Quick short introduction to myself, my name is Alexander Trost. I’m a sysadmin who loves automation, containers, coding in Go, playing games but also with new technologies.
I&rsquo;m currently working at Cloudical as a DevOps Engineer, helping companies move to the cloud and / or to container technologies (e.g., Docker, Kubernetes, etc).</p><h2 id=goal-of-the-training>Goal of the Training</h2><p>The training is going to show how simple it is to get started with containers. In this case Docker is used, as it is the most popular container toolchain and runtime right now.
After getting to know containers and Docker, we will hopefully realize that there is a need for some kind of <em>magical</em> orchestration layer to run applications and more in containers (in an orchestrated way). The important if you haven&rsquo;t noticed here is the orchestration of containers in an automated and orchestrated manner.</p><hr><h2 id=goal-of-the-day>Goal of the day</h2><p>Goal of the day is to look into Kubernetes itself by going over the architecture and components of a Kubernetes cluster. Besides that we will take a closer look at the configuration of kubelet and Kubernetes master components.
Then we will go over operational topics, like <a href=#monitoring>monitoring</a>, <a href=#logging>logging</a> and <a href=#storage>storage</a>.</p><hr><h2 id=architecture--components>Architecture + Components</h2><p>Information can also be found in a more compact format here: <a href=https://kubernetes.io/docs/concepts/overview/components/>Kubernetes Components Overview - Kubernetes</a>.</p><p>Remember this Kubernetes cluster architecture diagram?</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=kubernetes-cluster-architecture.png itemprop=contentUrl><img itemprop=thumbnail src=kubernetes-cluster-architecture.png width=1200px></a><figcaption><h4>Kubernetes Cluster architecture</h4></figcaption></figure><p>Now we go over each component in more detail.</p><h3 id=master-components>Master Components</h3><p>Even though they are called &ldquo;Master Components&rdquo; this doesn&rsquo;t mean they all need to run on the masters. For some components, e.g., etcd, it is recommended to run them on separate servers for better performance.</p><h4 id=etcd---the-brain>etcd - The brain</h4><blockquote><p><strong>REFERENCE</strong> <a href=https://github.com/etcd-io/etcd>GitHub etcd-io/etcd</a>.</p></blockquote><p>The etcd is the brain of a Kubernetes cluster. It is responsible for storing all the Kubernetes objects.
etcd in itself is a &ldquo;distributed reliable key-value store for most critical data of a distributed system&rdquo;. It uses the Raft for the clustering and failover logic. In the etcd documentation there is a whole documentation about the mechanism they have and what you can do with it, see <a href=https://etcd.readthedocs.io/en/latest/server-learner.html>etcd documentation - &ldquo;Cluster Membership&rdquo; Learner</a>.</p><p>Kubernetes uses etcd v3 API since some time now, so you should hopefully not come across any Kubernetes clusters that still use etcd v2 API.</p><blockquote><p><strong>NOTE</strong></p><p>Looking at the format in which Kubernetes stored objects with the etcd v2 API it is &ldquo;easier&rdquo; for humans to understand, but isn&rsquo;t as performant as the v3 API. Kubernetes stores data in protobuf format with etcd v3 API, enabling Kubernetes to be pretty fast with its objects.</p></blockquote><p>Running etcd can be a b*tch, but unless you throw away the etcd data every 5 minutes because you want refresh servers all the time you should not have too many problems.</p><blockquote><p><strong>TL;DR</strong> etcd is the data store of the Kubernetes objects. Can be a bottleneck of the Kubernetes cluster.</p></blockquote><p><code>etcdctl</code> is etcd&rsquo;s client tool which allows you to do certain tasks against an etcd cluster. With <code>etcdctl</code> you can create, edit and delete keys (objects) and many more things, but you don&rsquo;t want to do that unless you what you do. If you would edit objects you&rsquo;ll can end up with Kubernetes being unable to decode objects and thus possibly causing the Kubernetes cluster to fail sooner or later.</p><p>Ironically I have wrote a documentation page about doing exactly that, &ldquo;safely&rdquo; editing Kubernetes objects in etcd, see <a href=https://docs.edenmal.moe/kubernetes/etcd/editing-kubernetes-objects/>Editing Kubernetes objects in <code>etcd</code> - Edenmal Docs</a>.</p><p>From what people have told for bigger Kubernetes cluster installations (many nodes and / or many many users), the etcd can be a bottleneck depending on the (virtual) hardware.
To quote such a post from memory:</p><blockquote><p>&ldquo;etcd was getting slower and slower with more people using the cluster:</p></blockquote><ul><li>Checked the IO load on the Kubernetes master servers running etcd.</li><li>Moved etcd to dedicated servers.</li><li>Additionally put SSD or better under etcd data directory.</li><li>And bottleneck is gone!&rdquo;</li></ul><p>If you want to read about more such bigger Kubernetes cluster scaling cases, just search for &ldquo;Scaling Kubernetes to a million users&rdquo; / &ldquo;Issues scaling Kubernetes&rdquo;.</p><blockquote><p><strong>NOTE</strong> Most people will &ldquo;never&rdquo; run more than at max 100+ nodes per cluster.</p><p>Unless you are Google or have very ambitious plans like me :-D</p></blockquote><h4 id=kube-apiserver>kube-apiserver</h4><blockquote><p><strong>REFERENCE</strong> <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/>Kubernetes - kube-apiserver Reference</a>.</p></blockquote><p>When etcd is the brain of the Kubernetes cluster, the kube-apiserver is the ingestion system for information / obejcts so to speak of.
The kube-apiserver is the only component talking to the etcd directly, all other upcoming are always only talking to the kube-apiserver!</p><blockquote><p><strong>NOTE</strong></p><p>YES! All other components are talking through the kube-apiserver. The reason for that is simple, the kube-apiserver is the one instance that does authentication for users / ServiceAccounts*, validation (+ admission controlling) of objects.</p><p>*ServiceAccounts = are as the name implies &ldquo;accounts&rdquo; for services / robots, e.g., a token for your CI pipeline used for deployments to Kubernetes, controlled access for applications / operators to the Kubernetes API.</p></blockquote><p>Ever heard of fancy custom Kubernetes objects (CustomResourceDefinitions) like <code>Prometheus</code>? kube-apiserver takes care of registering their paths in its API.</p><h4 id=kube-controller-manager>kube-controller-manager</h4><blockquote><p><strong>REFERENCE</strong> <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/>Kubernetes - kube-controller-manager Reference</a>.</p></blockquote><p>kube-controller-manager is kind of the backbone as it uses the kube-apiserver to watch certain objects and the runs control loops to react on changes to them.
Examples of those control loops:</p><ul><li><code>Deployment</code> object has its <code>replicas</code> changed from <code>2</code> to <code>5</code>, the kube-controller-manager will then go ahead and update the current <code>ReplicaSet</code> for the <code>Deployment</code> and with that create the new Pods.</li><li>A <code>Node</code> has not been updated (keeped alive) by the server&rsquo;s kubelet since time n, kube-controller-manager will mark the node as <code>NotReady</code> and then after some additional time evict (delete) the Pods from the node so they are rescheduled.</li><li>Allocating IP CIDRs to each <code>Node</code> in the cluster (if enabled).</li><li>Many more jobs of keeping in control of what is going on in the Kubernetes cluster.</li></ul><h5 id=cloud-controller-manager---are-you-in-the-cloud>cloud-controller-manager - Are you in the cloud?</h5><blockquote><p><strong>REFERENCE</strong> <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/cloud-controller-manager/>Kubernetes - cloud-controller-manager Reference</a>.</p></blockquote><p>Is like the kube-controller-manager, but for the cloud. It is talking with the underlying cloud provider to provide certain services of the cloud provider to Kubernetes.</p><p>To quote the documentation, as there isn&rsquo;t really a better way to say it:</p><blockquote><ul><li>Node Controller: For checking the cloud provider to determine if a node has been deleted in the cloud after it stops responding</li></ul></blockquote><ul><li>Route Controller: For setting up routes in the underlying cloud infrastructure</li><li>Service Controller: For creating, updating and deleting cloud provider load balancers</li><li>Volume Controller: For creating, attaching, and mounting volumes, and interacting with the cloud provider to orchestrate volumes</li></ul><blockquote><p>- <a href=https://kubernetes.io/docs/concepts/overview/components/#cloud-controller-manager>Kubernetes Components Overview - cloud-controller-manager - Kubernetes</a></p></blockquote><p>It is basically a cloud support for the kube-controller-manager.</p><h4 id=kube-scheduler>kube-scheduler</h4><blockquote><p><strong>REFERENCE</strong> <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/>Kubernetes - kube-scheduler Reference</a>.</p></blockquote><p><em>*hoping that the name makes it obvious to what the kube-scheduler does*</em></p><p>Well.. the kube-scheduler schedules Pods in the Kubernetes cluster.
The kube-scheduler doesn&rsquo;t just throw dice but will also take available resources on the Nodes, &ldquo;placement&rdquo; options (Pod- and Node Affinity / AntiAffinity, Taints and Tolerations) and even if supported by the used storage the &ldquo;location&rdquo; of PersistentVolumes into account to find the best place for each Pod.</p><p>It doesn&rsquo;t do more but it also doesn&rsquo;t do less than that.</p><blockquote><p><strong>NOTE</strong></p><p>One thing that I want to note, but wouldn&rsquo;t recommend unless you know what you are doing. You can tell objects, like Deployments, StatefulSets and so on, which scheduler to use (<code>schedulerName</code> field). This means that you can run your own schedulers to assign Pods to Nodes on your own custom logic.</p></blockquote><h3 id=node-components>Node Components</h3><p>This section is about all the components needed for a server to be a function Node in a Kubernetes cluster.</p><h4 id=container-runtime-interface>Container Runtime Interface</h4><p>The Container Runtime Interface is an unified interface to talk to a Container Runtime (e.g., <a href=https://github.com/containerd/containerd>containerd</a>, <a href=https://cri-o.io>CRI-O</a>), for more insight on Kubernetes side, see <a href=https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/>Kubernetes Blog - Introducing Container Runtime Interface (CRI) in Kubernetes</a>.</p><p>Docker is also a Container Runtime that Kubernetes supports it, but right now this integration is directly through Docker (/ Moby) libraries instead of one unified interface which works for all container runtimes.</p><p>The point of different Container runtimes is, e.g., that Kata Containers is able to run containers in lightweight VMs to give (untrusted) workload more isolation.</p><p>It is needed on every Node that will run Pods in the end. Depending on how the Kubernetes cluster is installed, e.g., with kubeadm, all servers including the master servers can run containers (master servers are &ldquo;restricted&rdquo; (tainted) by default).</p><h4 id=kubelet>kubelet</h4><blockquote><p><strong>REFERENCE</strong> <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/>Kubernetes - kube-apiserver Reference</a>.</p></blockquote><p>Besides the <a href=#container-runtime-interface>Container Runtime of your choice</a>, the kubelet is the most important component on the Nodes (followed by the <a href=#kube-proxy>kube-proxy</a>).</p><p>The kubelet is the one and only component which is talking with the container runtime to create and run containers for a Pod.
For example the kubelet takes care of following tasks:</p><ul><li>Creating the containers with their shared (network) contexts.</li><li>Depending on the storage used, mounts and if needed formats the volumes for each Pod.</li><li>Besides PersistentVolumes mounting, it will also create the necessary volumes for ConfigMaps and Secrets.<ul><li>E.g., when using <code>emptyDir</code> is used with <code>medium: memory</code> it takes care of creating the in-memory directory.</li></ul></li></ul><p>Besides that for the storage aspect it will talk with the &ldquo;installed&rdquo; CSI (/ FlexVolume) drivers to get the storage mounted.</p><h4 id=kube-proxy>kube-proxy</h4><blockquote><p><strong>REFERENCE</strong> <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/>Kubernetes - kube-apiserver Reference</a>.</p></blockquote><p>You have heard about <code>Services</code> in Kubernetes? They have those cool <code>ClusterIP</code>s. The kube-proxy is the one that is setting up ipvs and iptables rules (depending on the &ldquo;mode&rdquo; chosen) so the Service <code>ClusterIPs</code> are reachable.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=kubernetes-networking-explained-service-ip-iptables-flow.svg itemprop=contentUrl><img itemprop=thumbnail src=kubernetes-networking-explained-service-ip-iptables-flow.svg width=1050px></a><figcaption><h4>Kubernetes - Service IP iptables Diagram</h4></figcaption></figure><p>(Originally taken from <a href=https://docs.edenmal.moe/kubernetes/networking/explained/#service-ip-iptables>Kubernetes Network Explained - Service IP iptables - Edenmal Docs</a>)</p><h4 id=cni--sdn>CNI / SDN</h4><p>This was mostly covered in the previous Network section, but to summarize:</p><p>It takes care of the network for the Pods started by the kubelet.
In most times the CNI will create a overlay network in &ldquo;mesh network between all Nodes&rdquo; style.</p><p>To name some CNIs:</p><ul><li><a href=https://github.com/coreos/flannel>CoreOS Flannel</a>: VXLAN Mesh network between the servers. The simplest network plugin that can span over more than one node.</li><li><a href=https://www.projectcalico.org/>Project Calico</a>: Either BGP + IP-IP or in form of <a href=https://docs.projectcalico.org/latest/getting-started/kubernetes/installation/flannel>Canal</a> using Flannel for the traffic between the nodes.</li><li>And many more <a href=https://kubernetes.io/docs/concepts/cluster-administration/networking/>Kubernetes - Cluster Networking</a> &mldr;</li></ul><p>Most CNIs are deployed through a DaemonSet that then runs the CNI in a container, which makes it easier to update and maintain.</p><h3 id=addons>Addons</h3><h4 id=dns>DNS</h4><blockquote><p>&ldquo;Isn&rsquo;t the problem always either network, DNS or both?&rdquo;</p></blockquote><p>A Kubernetes cluster relies heavily on DNS. This is because Kubernetes itself provides service discovery through the <code>Services</code> which get a DNS name. Reason for that is that if running in another cluster the <code>Service</code> ClusterIP could be different for whatever reason (e.g., different network ranges used).</p><p>This addon is most of the time running inside of Kubernetes. The DNS server used is <a href=https://coredns.io/>CoreDNS</a>, which is &ldquo;CoreDNS is a DNS server that chains plugins&rdquo;.
Should you run into performance issues with the DNS inside a Kubernetes cluster consider the following:</p><ul><li>Slow external name resolution?<ul><li>=> Check what upstream DNS servers are used in the CoreDNS configuration (<code>coredns</code> ConfigMap in <code>kube-system</code> namespace).</li></ul></li><li>Getting weird DNS timeouts of >= 5 seconds?<ul><li>=> See the following sources for information:<ul><li><a href=https://tech.xing.com/a-reason-for-unexplained-connection-timeouts-on-kubernetes-docker-abd041cf7e02>A reason for unexplained connection timeouts on Kubernetes/Docker by Maxime Lagresle - Xing Engineering blog</a></li><li><a href=https://github.com/kubernetes/kubernetes/issues/56903>GitHub kubernetes/kubernetes - Issue: DNS intermittent delays of 5s #56903</a></li></ul></li></ul></li></ul><h4 id=monitoring-logging-and-more>Monitoring, Logging and more</h4><p>These components / parts will be covered in the <a href=#operating-a-kubernetes-cluster>Operating a Kubernetes cluster</a> section.</p><hr><h2 id=operating-a-kubernetes-cluster>Operating a Kubernetes Cluster</h2><h3 id=what-is-an-operator-making-your-life-easier>What is an Operator? Making your life easier.</h3><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=jeopardy-kubernetes-operator-automation-question.png itemprop=contentUrl><img itemprop=thumbnail src=jeopardy-kubernetes-operator-automation-question.png width=750px></a><figcaption><h4>Jeopardy - Kubernetes 'What is an Operator' question</h4></figcaption></figure><p>An Operator is a pattern in Kubernetes. What are you doing when running and taking care of an application? You are operating the application.
Can you guess what an Operator (pattern) is doing in Kubernetes then?</p><p>An Operator takes care of running the application. You, the user, are just creating a custom object in Kubernetes, which has been defined by the operator creator. Based on that the operator reacts and, e.g., for Rook.io is creating each component of a Ceph cluster in Kubernetes through normal Deployment, StatefulSets and other objects.</p><p>Examples for operators:</p><ul><li><a href=https://rook.io/>Rook.io operators</a> - Run a whole Ceph cluster in Kubernetes, EdgeFS, NFS and more in Kubernetes with ease.</li><li><a href=https://github.com/coreos/prometheus-operator>CoreOS prometheus-operator</a> - Run and configure Prometheus easily through custom objects in Kubernetes.</li><li><a href=https://github.com/zalando/postgres-operator>Zalando postgres-operator</a> - Run Postgres Clusters in Kubernetes.</li><li>For more cool operators, checkout the <a href=https://operatorhub.io/>OperatorHub</a>.</li></ul><p>These operators allow admins and developers to reduce the time spent on getting things to run / running applications in Kubernetes.</p><blockquote><p><strong>NOTE</strong></p><p>This doesn&rsquo;t mean that you, the user doesn&rsquo;t need to know how to run, tune, maintain these applications. Operators are just automation and in the &ldquo;worst&rdquo; case manual intervention is the only help to fix bad issues unresolveable by automation.</p></blockquote><h3 id=monitoring>Monitoring</h3><p>Before going into the actual components, like Prometheus and Grafana that are used for monitoring, let&rsquo;s talk about how you can get &ldquo;agents&rdquo; / exporters on the servers to get, e.g., metrics, logs and other useful stuff. The way to do that achieve that are <code>DaemonSets</code>.</p><h4 id=daemonset>DaemonSet</h4><p>A DaemonSet is a way to run a Pod on any Node in the Kubernetes cluster.</p><p>Pods created through a DaemonSet are scheduled partially by the so called DaemonSet controller but also through the default scheduler since Kubernetes v1.12.
Meaning that you can specify certain placement options, like NodeAffinity, every Node in the Kubernetes cluster will get a Pod of a DaemonSet scheduled on it.</p><p>For more information on how DaemonSet Pods are scheduled, see <a href=https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#how-daemon-pods-are-scheduled>Kubernetes - DaemonSet - How Daemon Pods are Scheduled</a>.</p><p>This is perfect for, e.g., deployment of CNI plugin (Flannel, etc) and for monitoring, but also logging such an &ldquo;agent&rdquo; / exporter can be deployed easily.</p><h4 id=grafana>Grafana</h4><p>Haven&rsquo;t heard about <a href=https://grafana.com>Grafana</a> yet? You&rsquo;re in for a treat.</p><p>Grafana is the &ldquo;best&rdquo; tool for visualizing metrics.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=grafana-dashboard-k8s-resources.png itemprop=contentUrl><img itemprop=thumbnail src=grafana-dashboard-k8s-resources.png width=800px></a><figcaption><h4>Grafana Dashboard - Kubernetes Cluster Resources</h4></figcaption></figure><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=grafana-dashboard-ceph-cluster.png itemprop=contentUrl><img itemprop=thumbnail src=grafana-dashboard-ceph-cluster.png width=800px></a><figcaption><h4>Grafana Dashboard - Ceph Cluster Overview</h4></figcaption></figure><h4 id=prometheus>Prometheus</h4><p><a href=https://prometheus.io/>Prometheus</a> is the best cloud native for monitoring. Thanks to it, <a href=https://openmetrics.io/>OpenMetrics</a>, <a href=https://opencensus.io>OpenCensus</a> and many other projects have been sparked which allow to collect metrics and also do other things such as collecting traces.
Prometheus itself is only for metrics though.</p><p>Big differences to most other monitoring systems is that the Prometheus server is pulling metrics from <code>/metrics</code> (or other paths) endpoints. Another difference is that there is no high availability concept in point of sharing the data bwtween one or more instances.
If you want HA, for Prometheus you just run more than one instance and use, e.g., remote storage which writes to InfluxDB or other time serires databases for the data high availability aspect.</p><h3 id=logging>Logging</h3><h4 id=who-needs-logs-anyway-am-i-right>Who needs logs anyway, am I right?</h4><p>Logs are always a problem in the new cool container world.</p><p>The one application logs too much</p><h4 id=loki>Loki</h4><p><a href=https://grafana.com/loki>Loki</a> &ldquo;Prometheus-inspired logging for cloud natives.&rdquo;</p><p>An alternative to Elasticsearch, from the makers of <a href=#grafana>Grafana</a>.</p><h4 id=kibana>Kibana</h4><p><a href=https://www.elastic.co/products/kibana>Kibana</a> is made by the guys a tool which is especially good for looking at logs is Kibana. Grafana can also display logs, but Kibana is better in that regard through their simple search interface.
Sadly in Kibana I personally think the visualizations are not as good as in Grafana.</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=kibana-home-site.png itemprop=contentUrl><img itemprop=thumbnail src=kibana-home-site.png width=800px></a><figcaption><h4>Kibana Home Site</h4></figcaption></figure><h4 id=storing-logs>Storing Logs</h4><ul><li>EFK (Elasticsearch + Fluentd + Kibana)</li><li>ELK (Elasticsearch + Logstash + Kibana)</li></ul><p>There aren&rsquo;t really alternatives besides <a href=#loki>Loki</a>.</p><h3 id=storage>Storage</h3><h4 id=local-storage>Local Storage</h4><p>Is a feature to allow you to provide applications with storage from the nodes themselves for better performance.</p><blockquote><p><strong>WARNING</strong></p><p>The storage is not replicated in any way as it is local on the Node!</p></blockquote><p>Example: A Pod using Local Storage from Node A, will always be &ldquo;forced&rdquo; to run on Node A. Only when the user manually intervens the Pod would be &ldquo;moved&rdquo; to another Local Storage PersistentVolume.</p><p>This must be kept in mind when using Local Storage and already during planning.</p><h4 id=container-storage-interface-csi---in-memory-of-flexvolume>Container Storage Interface (CSI) - In memory of FlexVolume</h4><p>CSI allows storage providers to easily provide their storage to any software / platform through the CSI standard.</p><p>This is especially good for Kubernetes as this allows bugs in volume providers / drivers to be fixed faster, than if they would be in-tree in Kubernetes (in the Kubernetes code).</p><h4 id=rookio-ftw>Rook.io FTW!?</h4><p><a href=https://rook.io/>Rook.io</a></p><p>Rook can run storage providers for you through operators, e.g., Cassandra, Ceph, CockroachDB, EdgeFS, Minio, NFS and YugabyteDB.</p><h3 id=maintenance-tasks>Maintenance Tasks</h3><h4 id=upgrading-a-cluster-order>Upgrading a Cluster (Order)</h4><p>Any pre-upgrade processes documented / communicated must be applied before upgrading components.</p><ul><li>Master Components<ul><li><code>kube-apiserver</code></li><li><code>kube-controller-manager</code><ul><li>If used, <code>cloud-controller-manager</code>.</li></ul></li><li><code>kube-scheduler</code></li></ul></li><li>Node Components<ul><li><code>kubelet</code></li><li><code>kube-proxy</code></li></ul></li></ul><p>After that other components, like, e.g., CNI plugin, Monitoring, Operators.</p><h5 id=cordon-bleu-my-nodes>Cordon (Bleu) my Nodes</h5><p>Cordoning a Node makes it unschedulable (bleed out as no new Pods are scheduled on it then).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#44475a>kubectl cordon NODE_NAME
</span></span></span></code></pre></div><h5 id=drain-a-node>Drain a Node</h5><p>Draining a Node removes each Pod from a Node and marks it as cordoned (unschedulable). This is useful when, e.g., wanting to maintenance to a Node.</p><blockquote><p><strong>NOTE</strong></p><p>Be aware though that if users run applications in Kubernetes with <code>replicas: 1</code> they will have a service intrruption, due to Pods bein terminated and created on another Node.</p></blockquote><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#44475a>kubectl drain NODE_NAME
</span></span></span><span style=display:flex><span><span style=color:#44475a>kubectl drain --ignore-daemonsets=true NODE_NAME
</span></span></span><span style=display:flex><span><span style=color:#44475a>kubectl drain --ignore-daemonsets=true --delete-local-data=true NODE_NAME
</span></span></span><span style=display:flex><span><span style=color:#44475a></span># I would recommend adding the <span style=color:#f1fa8c>`</span>--timeout<span style=color:#ff79c6>=</span>0s<span style=color:#f1fa8c>`</span> flag with a decent timeout <span style=color:#8be9fd;font-style:italic>time</span>
</span></span></code></pre></div><hr><h2 id=summary-of-the-day>Summary of the Day</h2><p>If you are reading this, you have made it to the end of day #2. Well done, sir or madam, have a second cookie!</p><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=here-is-a-cookie.png itemprop=contentUrl><img itemprop=thumbnail src=here-is-a-cookie.png width=300px></a><figcaption><h4>Here is a Cookie (from Memegen)</h4></figcaption></figure><p>I hope everyone had a good time during the training&rsquo;s first day and has taken new knowledge with them already.
If you have any feedback about the training itself or the materials, please let me know in person or email me at <a href=mailto:me@galexrt.moe>me AT galexrt DOT moe</a>.</p><p>Have Fun!</p></article><footer class=post-footer><ul class=post-tags><li><a href=https://edenmal.moe/tags/Workshop><span class=tag>Workshop</span></a></li><li><a href=https://edenmal.moe/tags/Docker><span class=tag>Docker</span></a></li><li><a href=https://edenmal.moe/tags/Kubernetes><span class=tag>Kubernetes</span></a></li><li><a href=https://edenmal.moe/tags/Container><span class=tag>Container</span></a></li></ul><p class=post-copyright>© This page/post is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License, please give source if you wish to quote or reproduce.This post was published <strong>649</strong> days ago, content in the post may be inaccurate, even wrong now, please take risk yourself.</p></footer><script src=https://utteranc.es/client.js repo=galexrt/edenmal.moe issue-term=title label=blogpost theme=icy-dark crossorigin=anonymous async></script></section><footer class=site-footer><p>© 2017-2024 Alexander Trost</p><p>Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> with customized theme <a href=https://github.com/laozhu/hugo-nuo target=_blank rel=noopener>Nuo</a>.</p></footer><link rel=stylesheet href=https://edenmal.moe/styles/icofont.min.min.css><script async src=https://edenmal.moe/cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.min.120e30ad299b8d6548dd1fbb6ab1d45fb508bf080219df63e5ab9750b1241207.js integrity="sha256-Eg4wrSmbjWVI3R+7arHUX7UIvwgCGd9j5auXULEkEgc="></script><script type=text/x-mathjax-config>
  MathJax.Ajax.config.path["MathJax"] = "/cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5";
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      displayMath: [['$$','$$'], ['\\[','\\]']],
      processEscapes: true,
      processEnvironments: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      TeX: { equationNumbers: { autoNumber: "AMS" },
      extensions: ["AMSmath.js", "AMSsymbols.js"] }
    },
  });
</script><script type=text/x-mathjax-config>
  // Fix <code> tags after MathJax finishes running. This is a
  // hack to overcome a shortcoming of Markdown. Discussion at
  // https://github.com/mojombo/jekyll/issues/199
  MathJax.Hub.Queue(() => {
    MathJax.Hub.getAllJax().map(v => v.SourceElement().parentNode.className += ' has-jax');
  });
</script><script src=https://edenmal.moe/scripts/index.min.0375b6e63e18876ff8d4ad95bf8ef081176b6b994398d5bd22f5140f45565d37.js integrity="sha256-A3W25j4Yh2/41K2Vv47wgRdra5lDmNW9IvUUD0VWXTc="></script><script>"serviceWorker"in navigator&&navigator.serviceWorker.register("/service-worker.js").then(function(){console.log("[ServiceWorker] Registered")})</script></body></html>