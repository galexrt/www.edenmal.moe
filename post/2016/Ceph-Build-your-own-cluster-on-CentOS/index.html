<!doctype html><html lang=en-us><head><meta charset=utf-8><title>Ceph: Build your own cluster on CentOS | Edenmal - Sysadmin Garden of Eden</title>
<meta name=author content="Alexander Trost"><meta name=description content="Ceph is an object, a block and filesystem storage system."><meta name=twitter:site content="@galexrt"><meta name=twitter:title content="Ceph: Build your own cluster on CentOS | Edenmal - Sysadmin Garden of Eden"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://edenmal.moe/post/covers/Ceph_Logo_Stacked_RGB_120411_fa.png"><meta name=twitter:description content="Ceph is an object, a block and filesystem storage system."><meta property="og:type" content="article"><meta property="og:title" content="Ceph: Build your own cluster on CentOS"><meta property="og:description" content="Ceph is an object, a block and filesystem storage system."><meta property="og:url" content="https://edenmal.moe/post/2016/Ceph-Build-your-own-cluster-on-CentOS/"><meta property="og:image" content="https://edenmal.moe/post/covers/Ceph_Logo_Stacked_RGB_120411_fa.png"><meta name=generator content="Hugo 0.120.4"><link rel=canonical href=https://edenmal.moe/post/2016/Ceph-Build-your-own-cluster-on-CentOS/><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=format-detection content="telephone=no,email=no,adress=no"><meta http-equiv=Cache-Control content="no-transform"><meta name=robots content="index,follow"><meta name=referrer content="origin-when-cross-origin"><meta name=theme-color content="#4d4ac7"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="black"><meta name=apple-mobile-web-app-title content="Edenmal"><meta name=msapplication-tooltip content="Edenmal"><meta name=msapplication-navbutton-color content="#4d4ac7"><meta name=msapplication-TileColor content="#4d4ac7"><meta name=msapplication-TileImage content="/icons/icon-144x144.png"><link rel=icon href=https://edenmal.moe/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://edenmal.moe/icons/icon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://edenmal.moe/icons/icon-32x32.png><link rel=icon sizes=192x192 href=https://edenmal.moe/icons/icon-192x192.png><link rel=apple-touch-icon href=https://edenmal.moe/icons/icon-152x152.png><link rel=manifest href=https://edenmal.moe/manifest.json><link rel=preload href=https://edenmal.moe/styles/main-rendered.min.css as=style><link rel=preload href=https://edenmal.moe/images/avatar.png as=image><link rel=preload href=https://edenmal.moe/images/grey-prism.svg as=image><style>body{background-color:#f4f3f1;background-image:url(/images/grey-prism.svg);background-repeat:repeat;background-attachment:fixed}</style><link rel=stylesheet href=https://edenmal.moe/styles/main-rendered.min.css><link rel=stylesheet href=https://edenmal.moe/cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe.min.min.css><link rel=stylesheet href=https://edenmal.moe/cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/default-skin/default-skin.min.min.css><script src=https://edenmal.moe/cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe.min.min.819c76b4bba0f8b8c568f6e4860173f1d16e5046c01b008f4705ae823b5766e1.js integrity="sha256-gZx2tLug+LjFaPbkhgFz8dFuUEbAGwCPRwWugjtXZuE="></script><script src=https://edenmal.moe/cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.3/photoswipe-ui-default.min.min.2fcfc43be15566e9378af0e8247ba57fec8d4ee95c5dba15cf97e84d6c2115bc.js integrity="sha256-L8/EO+FVZuk3ivDoJHulf+yNTulcXboVz5foTWwhFbw="></script><script src=https://edenmal.moe/cdnjs.cloudflare.com/ajax/libs/jquery/1.12.4/jquery.min.min.77ec5e7d7f844e9b6c2413a352b48194493a65c75d94d5cd9679a225c2da773c.js integrity="sha256-d+xefX+ETptsJBOjUrSBlEk6ZcddlNXNlnmiJcLadzw="></script><script src=https://edenmal.moe/scripts/pswp-init.min.299a530b67d8be41667aa9ed78920e272e970b0ac8ab7540ba3a7c1382b52de2.js integrity="sha256-KZpTC2fYvkFmeqnteJIOJy6XCwrIq3VAujp8E4K1LeI="></script></head><body><div class=suspension><a role=button aria-label="Go to top" title="Go to top" class="to-top is-hide"><span class=icofont-caret-up aria-hidden=true></span></a>
<a role=button aria-label="Print page" title="Print page" class=print-page onclick=window.print()><span class=icofont-print aria-hidden=true></span></a></div><header class=site-header><a class=avatara href=https://edenmal.moe/><img class=avatar src=https://edenmal.moe/images/avatar.png alt=Avatar></a><h2 class=title><a href=https://edenmal.moe/>Edenmal</a></h2><p class=subtitle>Sysadmin Garden of Eden</p><button class=menu-toggle type=button aria-label="Main Menu" aria-expanded=false tab-index=0>
<span class=icofont-navigation-menu aria-hidden=true></span></button><nav class="site-menu collapsed"><h2 class=offscreen>Main Menu</h2><ul class=menu-list><li class="menu-item
is-active"><a href=https://edenmal.moe/>Home</a></li><li class=menu-item><a href=https://edenmal.moe/about/>About</a></li><li class=menu-item><a href=https://edenmal.moe/tags/>Tags</a></li><li class=menu-item><a href=https://edenmal.moe/events/>Events</a></li><li class=menu-item><a href=https://edenmal.moe/site-notice/>Impressum</a></li></ul></nav><nav class="social-menu collapsed"><h2 class=offscreen>Social Networks</h2><ul class=social-list><li class=social-item><a href=mailto:me@galexrt.moe title=Email aria-label=Email><span class=icofont-envelope aria-hidden=true></span></a></li><li class=social-item><a href=https://galexrt.moe/ rel=me title=Homepage aria-label=Homepage><span class=icofont-page aria-hidden=true></span></a></li><li class=social-item><a href=//github.com/galexrt rel=me title=GitHub aria-label=GitHub><span class=icofont-github aria-hidden=true></span></a></li><li class=social-item><a href=//twitter.com/galexrt rel=me title=Twitter aria-label=Twitter><span class=icofont-twitter aria-hidden=true></span></a></li><li class=social-item><a href=//www.linkedin.com/in/alexander-trost rel=me title=LinkedIn aria-label=LinkedIn><span class=icofont-linkedin aria-hidden=true></span></a></li><li class=social-item><a href=//www.xing.com/profile/Alexander_Trost18 rel=me title=XING aria-label=XING><span class=icofont-xing aria-hidden=true></span></a></li><li class=social-item><a rel=alternate type=application/rss+xml href=https://edenmal.moe/index.xml title=RSS aria-label=RSS><span class=icofont-rss aria-hidden=true></span></a></li></ul></nav></header><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><section class="main post-detail"><header class=post-header><h1 class=post-title>Ceph: Build your own cluster on CentOS</h1><p class=post-meta>Author Alexander Trost · Read Time 7 minutes · Created Fri Jul 29 09:29:00 2016 · Updated Fri Mar 25 21:33:36 2022</p></header><article class=post-content><div class=fullwidthimg><figure itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><a href=https://edenmal.moe/post/covers/Ceph_Logo_Stacked_RGB_120411_fa.png itemprop=contentUrl><img itemprop=thumbnail src=https://edenmal.moe/post/covers/Ceph_Logo_Stacked_RGB_120411_fa.png width=850px></a></figure></div><p>Ceph is an object, a block and filesystem storage system.</p><p>As you can see from the Ceph.com visitors map <a href="http://www.revolvermaps.com/?target=enlarge&amp;i=1lzi710tj7s&amp;color=80D2DC&amp;m=0">here</a>, people all around the world search for it. You can also see some metrics about the Ceph project at their metrics site <a href=http://metrics.ceph.com/>here</a>.</p><p>I&rsquo;m going to go over the installation of a two node Ceph storage cluster.
You can then build on those storage nodes and choose multiple storage types you want to use.</p><p>I personally recommend to use at least two nodes even for a test cluster!</p><h2 id=requirements>Requirements</h2><p>I assume that you have basic linux knowledge and at least two servers with CentOS7 up and running that can reach each other.</p><p>In my case my two example servers are having 2vCores and 4GB RAM. The OS used is CentOS7 (to be exact <code>CentOS Linux release 7.2.1511 (Core)</code>).
My examples servers for this tutorial have the following addresses:</p><ul><li><code>203.0.113.1 ceph-tutorial-node1</code></li><li><code>203.0.113.2 ceph-tutorial-node2</code></li></ul><p><em><strong>Please note</strong> the IPs used here are according to <a href=https://tools.ietf.org/html/rfc5737>RFC5737</a></em></p><p><strong>You have to decide from what server you want to deploy your cluster.</strong>
I use the first server <code>ceph-tutorial-node1</code> as the &ldquo;deployment&rdquo; server.</p><h2 id=preparing-your-servers-for-ceph>Preparing your servers for Ceph</h2><p>Let&rsquo;s begin first by adding the Epel and Ceph repository to all our used servers.
On all the servers we have, we need to add the Epel package repositories to the repository list.
The following commands add the Epel package repositories and the Epel repository verification key:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#44475a>sudo yum install -y yum-utils &amp;&amp; sudo yum-config-manager --add-repo https://dl.fedoraproject.org/pub/epel/7/x86_64/
</span></span></span><span style=display:flex><span><span style=color:#44475a>sudo yum install --nogpgcheck -y epel-release
</span></span></span><span style=display:flex><span><span style=color:#44475a>sudo rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7
</span></span></span><span style=display:flex><span><span style=color:#44475a>sudo rm /etc/yum.repos.d/dl.fedoraproject.org*
</span></span></span></code></pre></div><p>Now that our servers have the Epel repository added, we&rsquo;re finally going to add the Ceph repository.
Use your favorited editor to create a file <code>/etc/yum.repos.d/ceph.repo</code> and copy the following content into it:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ini data-lang=ini><span style=display:flex><span><span style=color:#ff79c6>[ceph-noarch]</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>name</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>Ceph noarch packages</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>baseurl</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>http://download.ceph.com/rpm-jewel/el7/noarch</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>enabled</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>1</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>gpgcheck</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>1</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>type</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>rpm-md</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>gpgkey</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>https://download.ceph.com/keys/release.asc</span>
</span></span></code></pre></div><p><strong>Please note</strong> You can replace <code>jewel</code> with the Ceph release version you want to install (current latest release is <code>jewel</code>) and <code>el7</code> with, depending on what CentOS version number you have, for example for CentOS7 it should be <code>el7</code> and for CentOS6 it should be <code>el6</code> (other CentOS versions don&rsquo;t have official rpm builds available and will not work!).</p><p>Now we have the Ceph repository in our repository list on all of our servers.
An important step is to configure NTP service on all servers to keep the time synchronized across the storage cluster, but I&rsquo;m not going to cover this.
Other important things to make sure, before continuing with the tutorial:</p><ul><li>You should use an user with password less login and sudo without password rights on details how to create an user for that see <a href=http://docs.ceph.com/docs/master/start/quick-start-preflight/#enable-password-less-ssh>Ceph Preflight Docs</a>, but it is <strong>not required</strong>.</li><li>Selinux needs to be set to <code>Permissive</code> during the installation process (Command to disable SElinux temporarly<code>sudo setenforce 0</code>).</li><li>By default firewalld is in place on most CentOS servers, you either have to open the specific ports or just disable the firewall completely (for more details see <a href=http://docs.ceph.com/docs/master/start/quick-start-preflight/#open-required-ports>Ceph Preflight Docs</a>).</li></ul><h2 id=preparing-the-ceph-deployment>Preparing the Ceph deployment</h2><p>Switch to your deployment server and install the Ceph deployment package <code>ceph-deploy</code> on the deployment server, in my case server <code>ceph-tutorial-node1</code>.
The first commands will update your package database and system. The second command will install the <code>ceph-deploy</code> package on your CentOS server:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#44475a>sudo yum clean all &amp;&amp; sudo yum update
</span></span></span><span style=display:flex><span><span style=color:#44475a>sudo yum install ceph-deploy
</span></span></span></code></pre></div><p>Still on the deployment server (in my case <code>ceph-tutorial-node1</code>) create a folder where files for the deployment will be stored. For example:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#44475a>mkdir ~/my-ceph-cluster
</span></span></span><span style=display:flex><span><span style=color:#44475a>cd ~/my-ceph-cluster
</span></span></span></code></pre></div><h2 id=deploying-ceph-storage-nodes>Deploying Ceph storage nodes</h2><p>To generate the basic Ceph cluster configuration I run for my two servers:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#44475a>ceph-deploy new ceph-tutorial-node1 ceph-tutorial-node2
</span></span></span></code></pre></div><p><strong>Please note</strong> Where ceph-tutorial-node1 is you deployment server.</p><p>Example output of the <code>ceph-deploy new ceph-tutorial-node1</code> command:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ini data-lang=ini><span style=display:flex><span><span style=color:#6272a4># ceph-deploy new ceph-tutorial-node1</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>[ceph_deploy.cli][INFO  ] Invoked (1.5.34): /usr/bin/ceph-deploy new ceph-tutorial-node1</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>[ceph_deploy.cli][INFO  ] ceph-deploy options:</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>[ceph_deploy.cli][INFO  ]  username                      : None</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>[ceph_deploy.cli][INFO  ]  func                          : &lt;function new at 0x1cfb6e0&gt;</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>[ceph_deploy.cli][INFO  ]  verbose                       : False</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>[ceph_deploy.cli][INFO  ]  overwrite_conf                : False</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>[ceph_deploy.cli][INFO  ]  quiet                         : False</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x1d56710&gt;</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>[ceph_deploy.cli][INFO  ]  cluster                       : ceph</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>[ceph_deploy.cli][INFO  ]  ssh_copykey                   : True</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[ceph_deploy.cli][INFO  ]  mon                           : [&#39;ceph-tutorial-node1&#39;]</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>[ceph_deploy.cli][INFO  ]  public_network                : None</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>[ceph_deploy.cli][INFO  ]  ceph_conf                     : None</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>[ceph_deploy.cli][INFO  ]  cluster_network               : None</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>[ceph_deploy.cli][INFO  ]  default_release               : False</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>[ceph_deploy.cli][INFO  ]  fsid                          : None</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>[ceph_deploy.new][DEBUG ] Creating new cluster named ceph</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>[ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>[ceph-tutorial-node1][DEBUG ] connected to host: ceph-tutorial-node1</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>[ceph-tutorial-node1][DEBUG ] detect platform information from remote host</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>[ceph-tutorial-node1][DEBUG ] detect machine type</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>[ceph-tutorial-node1][DEBUG ] find the location of an executable</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>[ceph-tutorial-node1][INFO  ] Running command: /usr/sbin/ip link show</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>[ceph-tutorial-node1][INFO  ] Running command: /usr/sbin/ip addr show</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[ceph-tutorial-node1][DEBUG ] IP addresses found: [&#39;203.0.113.1&#39;]</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>[ceph_deploy.new][DEBUG ] Resolving host ceph-tutorial-node1</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>[ceph_deploy.new][DEBUG ] Monitor ceph-tutorial-node1 at 203.0.113.1</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[ceph_deploy.new][DEBUG ] Monitor initial members are [&#39;ceph-tutorial-node1&#39;]</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[ceph_deploy.new][DEBUG ] Monitor addrs are [&#39;203.0.113.1&#39;]</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>[ceph_deploy.new][DEBUG ] Creating a random mon key...</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>[ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...</span>
</span></span><span style=display:flex><span><span style=color:#50fa7b>[ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...</span>
</span></span></code></pre></div><p>Running <code>ls</code>, we see the generated files in the directory:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span># ls
</span></span><span style=display:flex><span><span style=color:#44475a>ceph.conf  ceph-deploy-ceph.log  ceph.mon.keyring
</span></span></span></code></pre></div><p>Now we need to add an option to the bottom of the default Ceph configuration file <code>ceph.conf</code>.</p><p>The options is:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#44475a>osd pool default size = 2
</span></span></span></code></pre></div><p>The option sets the number of replications, for two nodes you <strong>have</strong> tp set it to <code>2</code> else the Ceph cluster will not become healthy.
You can also add other options here, but as this is just a basic tutorial on how to create a Ceph cluster I&rsquo;m not covering other options than the default.</p><p>Now that our Ceph configuration <code>ceph.conf</code> is ready we are going to install Ceph on all servers and create our initial cluster monitor.
The command for deploying Ceph onto my two servers and creating the monitor server on your server chosen above, in my case <code>ceph-tutorial-node1</code> the commands look like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#44475a>ceph-deploy install --no-adjust-repos ceph-tutorial-node1 ceph-tutorial-node2
</span></span></span><span style=display:flex><span><span style=color:#44475a>ceph-deploy mon create-initial
</span></span></span></code></pre></div><p><strong>Errors</strong> If the command fails, you need to apply a bug fix with the command <code>sed -i '78s/allow \*/allow/g' /usr/lib/python2.7/site-packages/ceph_deploy/gatherkeys.py</code>. The bug can be seen <a href=http://tracker.ceph.com/issues/16443>here</a>).</p><p><strong>Please note</strong> If the last command exits with errors, check the connectivity of your servers and firewall between the servers (if a firewall is in place between the servers, make sure it allows the Ceph ports, ports: TCP Ceph mon(itor) 6789, Ceph OSDs 6800:7300 (for more information see <a href=http://docs.ceph.com/docs/master/start/quick-start-preflight/#open-required-ports>Ceph Preflight Docs</a>)).</p><h2 id=creating-the-storage-nodes>Creating the storage nodes</h2><p>Use ssh to connect to your servers that you want to be storage nodes and create a directory for the storaged data.</p><p>In my case:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#44475a>ssh ceph-tutorial-node1
</span></span></span><span style=display:flex><span><span style=color:#44475a>sudo mkdir /var/local/osd
</span></span></span><span style=display:flex><span><span style=color:#44475a>exit
</span></span></span><span style=display:flex><span><span style=color:#44475a></span>
</span></span><span style=display:flex><span><span style=color:#44475a>ssh ceph-tutorial-node2
</span></span></span><span style=display:flex><span><span style=color:#44475a>sudo mkdir /var/local/osd
</span></span></span><span style=display:flex><span><span style=color:#44475a>exit
</span></span></span></code></pre></div><p>Back on your deployment server (in my case <code>ceph-tutorial-node1</code>) we are going to prepare the storage nodes with the following command:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#44475a>ceph-deploy osd prepare {ceph-node}:/path/to/directory
</span></span></span></code></pre></div><p>For my two servers I run:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#44475a>ceph-deploy osd prepare ceph-tutorial-node1:/var/local/osd ceph-tutorial-node2:/var/local/osd
</span></span></span></code></pre></div><p>The <code>ceph-deploy osd prepare</code> prepares the storage location and sets up the Ceph OSD in the background.
The only thing we have to do now is to activate the storage locations (Ceph OSDs) using the activate command:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#44475a>ceph-deploy osd activate {ceph-node}:/path/to/directory
</span></span></span></code></pre></div><p>For my two servers I run:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#44475a>ceph-deploy osd activate ceph-tutorial-node1:/var/local/osd ceph-tutorial-node2:/var/local/osd
</span></span></span></code></pre></div><h2 id=finalizing-your-ceph-cluster>Finalizing your Ceph cluster</h2><p>Copy the Ceph admin key to the servers you want to be able to configure your Ceph cluster from with the below command:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#44475a>ceph-deploy admin {deployment-node}
</span></span></span></code></pre></div><p><strong>Please note</strong> Where <code>{deployment-node}</code> is your server you ran all the commands on.</p><p>To check if you Ceph cluster is healthy use the health command:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span># ceph health
</span></span><span style=display:flex><span><span style=color:#44475a>HEALTH_OK
</span></span></span></code></pre></div><p><strong>Please note</strong> If it shows something else than <code>HEALTH_OK</code> your cluster is unhealthy and may not even work properly.</p><p>That&rsquo;s all. Now you have a working Ceph storage cluster.</p><h2 id=troubleshooting>Troubleshooting</h2><h3 id=log-file-location>Log file location</h3><p>In case of other problems, all Ceph log files are located under <code>/var/log/ceph/</code>.</p><p><strong>Example</strong>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span># ls -hl /var/log/ceph/
</span></span><span style=display:flex><span><span style=color:#44475a>total 340K
</span></span></span><span style=display:flex><span><span style=color:#44475a>-rw------- 1 root root 126K Aug  8 13:24 ceph.log
</span></span></span><span style=display:flex><span><span style=color:#44475a>-rw-r--r-- 1 root root 196K Aug  8 13:25 ceph-mon.ceph-tutorial-node1.log
</span></span></span><span style=display:flex><span><span style=color:#44475a>-rw-r--r-- 1 root root 5.2K Aug  8 09:26 ceph-osd.0.log
</span></span></span></code></pre></div><h3 id=clock-skew-error>&ldquo;Clock skew&rdquo; error</h3><p>If the health command returns a message like <code>clock skew detected</code>, then your clock on the server(s) is not synced. Please install and configure an NTP server and client(s).</p><h2 id=summary>Summary</h2><p>This should help you get started with installing/running a Ceph cluster on CentOS.
If you have issues, please refer to the official <code>ceph-deploy</code> documentation her: <a href=http://docs.ceph.com/ceph-deploy/docs/>ceph-deploy - Deploy Ceph with minimal infrastructure - ceph-deploy 2.0.0 documentation</a> and <a href=http://docs.ceph.com/docs/master/rados/deployment/>Ceph Deployment - Ceph Documentation</a>.</p><p>Have Fun!</p></article><footer class=post-footer><ul class=post-tags><li><a href=https://edenmal.moe/tags/Ceph><span class=tag>Ceph</span></a></li></ul><p class=post-copyright>© This page/post is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License, please give source if you wish to quote or reproduce.This post was published <strong>726</strong> days ago, content in the post may be inaccurate, even wrong now, please take risk yourself.</p></footer><script src=https://utteranc.es/client.js repo=galexrt/edenmal.moe issue-term=title label=blogpost theme=icy-dark crossorigin=anonymous async></script></section><footer class=site-footer><p>© 2017-2024 Alexander Trost</p><p>Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> with customized theme <a href=https://github.com/laozhu/hugo-nuo target=_blank rel=noopener>Nuo</a>.</p></footer><link rel=stylesheet href=https://edenmal.moe/styles/icofont.min.min.css><script async src=https://edenmal.moe/cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.min.120e30ad299b8d6548dd1fbb6ab1d45fb508bf080219df63e5ab9750b1241207.js integrity="sha256-Eg4wrSmbjWVI3R+7arHUX7UIvwgCGd9j5auXULEkEgc="></script><script type=text/x-mathjax-config>
  MathJax.Ajax.config.path["MathJax"] = "/cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5";
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      displayMath: [['$$','$$'], ['\\[','\\]']],
      processEscapes: true,
      processEnvironments: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      TeX: { equationNumbers: { autoNumber: "AMS" },
      extensions: ["AMSmath.js", "AMSsymbols.js"] }
    },
  });
</script><script type=text/x-mathjax-config>
  // Fix <code> tags after MathJax finishes running. This is a
  // hack to overcome a shortcoming of Markdown. Discussion at
  // https://github.com/mojombo/jekyll/issues/199
  MathJax.Hub.Queue(() => {
    MathJax.Hub.getAllJax().map(v => v.SourceElement().parentNode.className += ' has-jax');
  });
</script><script src=https://edenmal.moe/scripts/index.min.0375b6e63e18876ff8d4ad95bf8ef081176b6b994398d5bd22f5140f45565d37.js integrity="sha256-A3W25j4Yh2/41K2Vv47wgRdra5lDmNW9IvUUD0VWXTc="></script><script>"serviceWorker"in navigator&&navigator.serviceWorker.register("/service-worker.js").then(function(){console.log("[ServiceWorker] Registered")})</script></body></html>